{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1adb9e0-7353-4aad-8ab3-a360c55d11f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.39.3\n",
      "  Using cached transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
      "Collecting safetensors==0.4.2\n",
      "  Using cached safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.3) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.3) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.3) (1.24.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.3) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.3) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.3) (2025.7.34)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.3) (2.32.2)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.39.3)\n",
      "  Using cached tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.3) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.3) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.3) (4.11.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.3) (1.1.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.3) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.3) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.3) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.3) (2024.2.2)\n",
      "Using cached transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
      "Using cached safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Installing collected packages: safetensors, tokenizers, transformers\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.6.2\n",
      "    Uninstalling safetensors-0.6.2:\n",
      "      Successfully uninstalled safetensors-0.6.2\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.4\n",
      "    Uninstalling tokenizers-0.21.4:\n",
      "      Successfully uninstalled tokenizers-0.21.4\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.55.2\n",
      "    Uninstalling transformers-4.55.2:\n",
      "      Successfully uninstalled transformers-4.55.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "accelerate 1.10.0 requires safetensors>=0.4.3, but you have safetensors 0.4.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed safetensors-0.4.2 tokenizers-0.15.2 transformers-4.39.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m‚úÖ torch: 2.8.0+cu128\n",
      "‚úÖ transformers: 4.39.3\n",
      "‚úÖ safetensors: 0.4.2\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for notebook environment\n",
    "!pip install pandas==2.1.3 numpy==1.26.3 matplotlib seaborn scikit-learn torch torchvision torchaudio transformers==4.44.2 safetensors==0.4.2 datasets optuna wandb sentencepiece accelerate==0.33.0 evaluate --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a83be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1386bd77-f21c-432a-acef-435c3f17dc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing PyTorch checkpoint loading...\n",
      "\n",
      "1. Testing DeBERTa checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at agentlans/deberta-v3-base-tweet-sentiment and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DeBERTa checkpoint loaded successfully!\n",
      "   Model type: DebertaV2ForSequenceClassification\n",
      "   Number of parameters: 184,425,989\n",
      "\n",
      "2. Testing RoBERTa checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RoBERTa checkpoint loaded successfully!\n",
      "   Model type: RobertaForSequenceClassification\n",
      "   Number of parameters: 124,649,477\n",
      "\n",
      "==================================================\n",
      "Checkpoint test completed!\n"
     ]
    }
   ],
   "source": [
    "# Quick test to verify PyTorch checkpoints load correctly\n",
    "def test_checkpoints():\n",
    "    print(\"Testing PyTorch checkpoint loading...\")\n",
    "\n",
    "    # Test DeBERTa checkpoint\n",
    "    try:\n",
    "        print(\"\\n1. Testing DeBERTa checkpoint...\")\n",
    "        deberta_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"agentlans/deberta-v3-base-tweet-sentiment\",\n",
    "            num_labels=5,ignore_mismatched_sizes=True\n",
    "        )\n",
    "        ckpt = torch.load(\n",
    "            \"/storage/yagel/ADL/checkpoints/deberta_hp_tuning_study/best_model/best_model.pt\",\n",
    "            map_location=\"cpu\"\n",
    "        )\n",
    "        deberta_model.load_state_dict(ckpt['model_state_dict'], strict=False)\n",
    "        print(\"‚úÖ DeBERTa checkpoint loaded successfully!\")\n",
    "        print(f\"   Model type: {type(deberta_model).__name__}\")\n",
    "        print(f\"   Number of parameters: {sum(p.numel() for p in deberta_model.parameters()):,}\")\n",
    "        del deberta_model\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå DeBERTa checkpoint failed: {e}\")\n",
    "\n",
    "    # Test RoBERTa checkpoint\n",
    "    try:\n",
    "        print(\"\\n2. Testing RoBERTa checkpoint...\")\n",
    "        roberta_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "            num_labels=5,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        ckpt = torch.load(\n",
    "            \"/storage/yagel/ADL/checkpoints/roberta_hp_tuning_study/best_model/best_model.pt\",\n",
    "            map_location=\"cpu\"\n",
    "        )\n",
    "        roberta_model.load_state_dict(ckpt['model_state_dict'], strict=False)\n",
    "        print(\"‚úÖ RoBERTa checkpoint loaded successfully!\")\n",
    "        print(f\"   Model type: {type(roberta_model).__name__}\")\n",
    "        print(f\"   Number of parameters: {sum(p.numel() for p in roberta_model.parameters()):,}\")\n",
    "        del roberta_model\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå RoBERTa checkpoint failed: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Checkpoint test completed!\")\n",
    "\n",
    "# Run the test\n",
    "test_checkpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dedaf9-ee20-4b0b-bdc3-96e9d4501835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading HuggingFace checkpoints for RoBERTa and DeBERTa\n",
    "\n",
    "def test_hf_checkpoints():\n",
    "    print(\"Testing HuggingFace checkpoints...\")\n",
    "\n",
    "    # Test DeBERTa HF checkpoint\n",
    "    try:\n",
    "        print(\"\\n1. Testing DeBERTa HF checkpoint...\")\n",
    "        deberta_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"/storage/yagel/ADL/deberta_results_HF/deberta_trial_4/checkpoint-4950\"\n",
    "        )\n",
    "        print(\"‚úÖ DeBERTa HF checkpoint loaded successfully!\")\n",
    "        print(f\"   Model type: {type(deberta_model).__name__}\")\n",
    "        print(f\"   Number of parameters: {sum(p.numel() for p in deberta_model.parameters()):,}\")\n",
    "        del deberta_model\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå DeBERTa HF checkpoint failed: {e}\")\n",
    "\n",
    "    # Test RoBERTa HF checkpoint\n",
    "    try:\n",
    "        print(\"\\n2. Testing RoBERTa HF checkpoint...\")\n",
    "        roberta_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"/storage/yagel/ADL/roberta_results_HF/trial_7/checkpoint-18000\"\n",
    "        )\n",
    "        print(\"‚úÖ RoBERTa HF checkpoint loaded successfully!\")\n",
    "        print(f\"   Model type: {type(roberta_model).__name__}\")\n",
    "        print(f\"   Number of parameters: {sum(p.numel() for p in roberta_model.parameters()):,}\")\n",
    "        del roberta_model\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå RoBERTa HF checkpoint failed: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"HF Checkpoint test completed!\")\n",
    "\n",
    "# Run the test\n",
    "test_hf_checkpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "132d9b9b-3718-4da7-9daa-7074f9822d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HF Model 1 - DeBERTa...\n",
      "Loading HF Model 2 - RoBERTa...\n",
      "Loading PT Model 1 - DeBERTa (fine-tuned)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at agentlans/deberta-v3-base-tweet-sentiment and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PT Model 2 - RoBERTa (fine-tuned)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4 models: ['hf_deberta', 'hf_roberta', 'pt_deberta', 'pt_roberta']\n"
     ]
    }
   ],
   "source": [
    "def load_models():\n",
    "    models = {}\n",
    "\n",
    "    # Model 1: HuggingFace checkpoint 1 - DeBERTa (from checkpoint directory)\n",
    "    print(\"Loading HF Model 1 - DeBERTa...\")\n",
    "    models['hf_deberta'] = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"/storage/yagel/ADL/deberta_results_HF/deberta_trial_4/checkpoint-4950\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # Model 2: HuggingFace checkpoint 2 - RoBERTa (from checkpoint directory)\n",
    "    print(\"Loading HF Model 2 - RoBERTa...\")\n",
    "    models['hf_roberta'] = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"/storage/yagel/ADL/roberta_results_HF/trial_7/checkpoint-18000\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # Model 3: PyTorch checkpoint 1 - DeBERTa (fine-tuned)\n",
    "    print(\"Loading PT Model 1 - DeBERTa (fine-tuned)...\")\n",
    "    models['pt_deberta'] = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"agentlans/deberta-v3-base-tweet-sentiment\",\n",
    "        num_labels=5,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    deberta_ckpt = torch.load(\"/storage/yagel/ADL/checkpoints/deberta_hp_tuning_study/best_model/best_model.pt\", map_location=DEVICE)\n",
    "    models['pt_deberta'].load_state_dict(deberta_ckpt['model_state_dict'], strict=False)\n",
    "    models['pt_deberta'] = models['pt_deberta'].to(DEVICE)\n",
    "\n",
    "    # Model 4: PyTorch checkpoint 2 - RoBERTa (fine-tuned)\n",
    "    print(\"Loading PT Model 2 - RoBERTa (fine-tuned)...\")\n",
    "    models['pt_roberta'] = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "        num_labels=5,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    roberta_ckpt = torch.load(\"/storage/yagel/ADL/checkpoints/roberta_hp_tuning_study/best_model/best_model.pt\", map_location=DEVICE)\n",
    "    models['pt_roberta'].load_state_dict(roberta_ckpt['model_state_dict'], strict=False)\n",
    "    models['pt_roberta'] = models['pt_roberta'].to(DEVICE)\n",
    "\n",
    "    return models\n",
    "\n",
    "# Load all models\n",
    "models = load_models()\n",
    "print(f\"Loaded {len(models)} models: {list(models.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de6b07b1-34b0-4bfd-8f4f-cbab1ec16072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes - Train: 28800, Val: 12343, Test: 3798\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"data/train_df.csv\")\n",
    "eval_df  = pd.read_csv(\"data/eval_df.csv\")\n",
    "test_df  = pd.read_csv(\"data/test_df.csv\")\n",
    "\n",
    "# ensure int labels, keep your columns\n",
    "for df in (train_df, eval_df, test_df):\n",
    "    df[\"label\"] = pd.to_numeric(df[\"label\"], errors=\"coerce\").astype(int)\n",
    "    assert df[\"label\"].between(0, 4).all(), f\"Labels out of range in {df.shape}: {set(df['label'])}\"\n",
    "\n",
    "train_df = train_df[[\"CleanTweet\",\"label\"]].reset_index(drop=True)\n",
    "eval_df  = eval_df[[\"CleanTweet\",\"label\"]].reset_index(drop=True)\n",
    "test_df  = test_df[[\"CleanTweet\",\"label\"]].reset_index(drop=True)\n",
    "\n",
    "class TweetsDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=256):\n",
    "        self.texts = dataframe[\"CleanTweet\"].tolist()\n",
    "        self.labels = dataframe[\"label\"].astype(int).tolist()  # ensure plain ints\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),       # squeeze batch dim only\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"agentlans/deberta-v3-base-tweet-sentiment\", use_fast=False)\n",
    "\n",
    "train_dataset = TweetsDataset(train_df, tokenizer)\n",
    "val_dataset   = TweetsDataset(eval_df,  tokenizer)\n",
    "test_dataset  = TweetsDataset(test_df,  tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=5, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=5, shuffle=False)\n",
    "\n",
    "print(f\"Dataset sizes - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e6839d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at agentlans/deberta-v3-base-tweet-sentiment and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint metadata - Study: deberta_hp_tuning_study\n",
      "Best trial: 0, Best accuracy: 0.7409867941343271\n",
      "Model loaded from: checkpoints/deberta_hp_tuning_study/best_model/best_model.pt\n",
      "Model loaded on cuda\n",
      "Model type: DebertaV2ForSequenceClassification\n",
      "Number of parameters: 184,425,989\n"
     ]
    }
   ],
   "source": [
    "# Load model from checkpoint\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load base DeBERTa-v3 tweet sentiment model first, then load your trained weights\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'agentlans/deberta-v3-base-tweet-sentiment',  # CHANGE: Update to match your model architecture (e.g., 'bert-base-uncased', 'roberta-base')\n",
    "    num_labels=5,              # CHANGE: Update number of labels to match your task\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Load your trained checkpoint weights\n",
    "checkpoint_path = \"checkpoints/deberta_hp_tuning_study/best_model/best_model.pt\"  #manual code  CHANGE: Update path to your checkpoint file\n",
    "checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "\n",
    "# Extract model weights from checkpoint metadata\n",
    "if 'model_state_dict' in checkpoint:\n",
    "    model_weights = checkpoint['model_state_dict']\n",
    "    print(f\"Loaded checkpoint metadata - Study: {checkpoint.get('study_name', 'N/A')}\")\n",
    "    print(f\"Best trial: {checkpoint.get('study_best_trial', 'N/A')}, Best accuracy: {checkpoint.get('best_val_accuracy', 'N/A')}\")\n",
    "else:\n",
    "    model_weights = checkpoint\n",
    "\n",
    "model.load_state_dict(model_weights)\n",
    "model.to(DEVICE)\n",
    "\n",
    "print(f\"Model loaded from: {checkpoint_path}\")\n",
    "print(f\"Model loaded on {DEVICE}\")\n",
    "print(f\"Model type: {type(model).__name__}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec0e0a4c-58c1-4bc2-8651-348487b96c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CleanTweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Panic buying hits NewYork City as anxious shop...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>toiletpaper dunnypaper coronavirus coronavirus...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          CleanTweet  label\n",
       "0  TRENDING: New Yorkers encounter empty supermar...      0\n",
       "1  When I couldn't find hand sanitizer at Fred Me...      3\n",
       "2  Find out how you can protect yourself and love...      4\n",
       "3  Panic buying hits NewYork City as anxious shop...      1\n",
       "4  toiletpaper dunnypaper coronavirus coronavirus...      2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2ef23b",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a61891-5531-4963-8308-3caa5be4ac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to measure model size in KB\n",
    "def get_model_size(model, filename=\"temp.pth\"):\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    size = os.path.getsize(filename) / 1024  # KB\n",
    "    # Clean up temporary file\n",
    "    os.remove(filename)\n",
    "    return size\n",
    "\n",
    "# Function to evaluate accuracy\n",
    "def evaluate_accuracy(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}  # Move everything to the device\n",
    "\n",
    "            outputs = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "            predictions = outputs.logits.argmax(dim=1)\n",
    "            correct += (predictions == batch[\"labels\"]).sum().item()\n",
    "            total += batch[\"labels\"].size(0)\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "# --- Main test loop ---\n",
    "def quantization_sweep(model, test_loader):\n",
    "    results = []\n",
    "\n",
    "    # Original FP32 model on GPU\n",
    "    model.eval()\n",
    "    fp32_size = get_model_size(model, \"fp32.pth\")\n",
    "    fp32_acc = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "    results.append((\"FP32 (baseline)\", fp32_size, fp32_acc, 0.0))\n",
    "\n",
    "    # INT8 Dynamic quantization (Linear layers only)\n",
    "    # Move model to CPU for quantization\n",
    "    model_cpu = model.cpu()\n",
    "    int8_model = quantize_dynamic(model_cpu, {nn.Linear}, dtype=torch.qint8)\n",
    "    int8_size = get_model_size(int8_model, \"int8.pth\")\n",
    "    \n",
    "    # Evaluate quantized model on CPU (quantized models can't run on GPU)\n",
    "    int8_acc = evaluate_accuracy(int8_model, test_loader, torch.device('cpu'))\n",
    "    results.append((\"INT8 Linear\", int8_size, int8_acc, 100*(1 - int8_size/fp32_size)))\n",
    "\n",
    "    # Move original model back to GPU\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"{'Format':<15} {'Size (KB)':<12} {'Accuracy':<10} {'% Size Saved':<12}\")\n",
    "    print(\"-\" * 55)\n",
    "    for name, size, acc, saving in results:\n",
    "        print(f\"{name:<15} {size:<12.1f} {acc:<10.4f} {saving:<12.1f}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30d3769-03d1-4c23-aee9-68a799b34b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DeBERTA manual code (pt checkpoint)\n",
    "model.to(DEVICE)\n",
    "results = quantization_sweep(model, test_loader)\n",
    "print(\"Quantization results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f565593a-70cf-4f22-aee3-04187cf55647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RoBERTA manual (pt checkpoint)\n",
    "\n",
    "\n",
    "# Paths & backbone\n",
    "roberta_backbone = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "roberta_ckpt     = \"checkpoints/roberta_hp_tuning_study/best_model/best_model.pt\"\n",
    "\n",
    "# Build loader with the **matching tokenizer**\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_backbone, use_fast=True)\n",
    "test_dataset = TweetsDataset(test_df, roberta_tokenizer, max_length=256)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Optional: sanity on label range for 5 classes\n",
    "assert test_df[\"label\"].between(0, 4).all(), \"Labels must be in [0..4] for num_labels=5.\"\n",
    "invalid_labels = test_df[~test_df[\"label\"].between(0, 4)]\n",
    "if not invalid_labels.empty:\n",
    "    print(\"‚ùå Invalid labels found:\\n\", invalid_labels)\n",
    "    raise ValueError(\"Test set contains invalid labels.\")\n",
    "\n",
    "# Load model + weights\n",
    "roberta_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    roberta_backbone, num_labels=5, ignore_mismatched_sizes=True\n",
    ")\n",
    "state = torch.load(roberta_ckpt, map_location=DEVICE)\n",
    "roberta_model.load_state_dict(state[\"model_state_dict\"])\n",
    "roberta_model.eval()\n",
    "roberta_model = roberta_model.to(DEVICE)\n",
    "\n",
    "# Sanity check: token IDs fit vocab\n",
    "batch = next(iter(test_loader))\n",
    "max_id = int(batch[\"input_ids\"].max())\n",
    "vocab_sz = roberta_model.roberta.embeddings.word_embeddings.num_embeddings\n",
    "print(f\"Max token id = {max_id} | vocab size = {vocab_sz}\")\n",
    "assert max_id < vocab_sz, \"Token ID exceeds model vocab size ‚Äì tokenizer/model mismatch.\"\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "roberta_model.to(DEVICE)\n",
    "results = quantization_sweep(roberta_model, test_loader)\n",
    "\n",
    "print(\"RoBERTa (PT) Quantization results:\", results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d012488f-415c-4943-b6e8-1922b9e15ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# DeBERTa: HF checkpoint + sweep\n",
    "# ===============================\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# DeBERTa paths and backbone\n",
    "deberta_backbone = \"agentlans/deberta-v3-base-tweet-sentiment\"  # Or your custom one if different\n",
    "\n",
    "# Tokenizer and loader\n",
    "deberta_tokenizer = AutoTokenizer.from_pretrained(deberta_backbone, use_fast=True)\n",
    "test_dataset = TweetsDataset(test_df, deberta_tokenizer, max_length=256)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Sanity check: label range\n",
    "assert test_df[\"label\"].between(0, 4).all(), \"Labels must be in [0..4] for num_labels=5.\"\n",
    "\n",
    "# Load model\n",
    "deberta_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"deberta_results_HF/deberta_trial_4/checkpoint-4950\", num_labels=5\n",
    ")\n",
    "\n",
    "\n",
    "# Run sweep on CPU\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "deberta_model.to(DEVICE)\n",
    "results = quantization_sweep(deberta_model, test_loader)\n",
    "print(\"DeBERTa (HF) Quantization results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0058696-48a7-47fb-8d71-837eb03ebdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# RoBERTa: HF CHECKPOINT + SWEEP\n",
    "# ===============================\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Tokenizer from base model\n",
    "roberta_backbone = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_backbone, use_fast=True)\n",
    "\n",
    "# Loader\n",
    "test_dataset = TweetsDataset(test_df, roberta_tokenizer, max_length=256)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Sanity check\n",
    "assert test_df[\"label\"].between(0, 4).all(), \"Labels must be in [0..4] for num_labels=5.\"\n",
    "\n",
    "# ‚úÖ Load model from local HF checkpoint\n",
    "roberta_ckpt = \"roberta_results_HF/trial_7/checkpoint-18000\"\n",
    "roberta_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    roberta_ckpt,\n",
    "    num_labels=5,\n",
    "    local_files_only=True  # <<< this tells HF to treat the path as local\n",
    ")\n",
    "\n",
    "# Run sweep\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "roberta_model.to(DEVICE)\n",
    "results = quantization_sweep(roberta_model, test_loader)\n",
    "\n",
    "print(\"RoBERTa (HF checkpoint) Quantization results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146a39f8-b648-400d-aa41-5d6854e796d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5336d7c1",
   "metadata": {},
   "source": [
    "# Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27a4842-5f67-43bc-b4d1-8a9484aff931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def pruning_tradeoff_sweep_cpu(model, test_loader, amounts=None):\n",
    "    \"\"\"\n",
    "    CPU-only pruning sweep with results table and plot\n",
    "    \n",
    "    Args:\n",
    "        model: Input model to prune\n",
    "        test_loader: DataLoader for evaluation\n",
    "        amounts: List of pruning amounts (default: 10%, 20%, 30%, 40%, 50%)\n",
    "    \n",
    "    Returns:\n",
    "        results: List of dictionaries with pruning results\n",
    "    \"\"\"\n",
    "    if amounts is None:\n",
    "        amounts = [0.1, 0.2, 0.3, 0.4, 0.5]  # 10%, 20%, 30%, 40%, 50%\n",
    "    \n",
    "    print(\"üî¨ Starting CPU Pruning Sweep\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for amount in amounts:\n",
    "        print(f\"\\nüìä Testing pruning amount: {amount*100:.0f}%\")\n",
    "        \n",
    "        # Create a copy of the model for this pruning amount\n",
    "        model_to_prune = copy.deepcopy(model)\n",
    "        \n",
    "        # Get parameters to prune (all Linear layers)\n",
    "        parameters_to_prune = []\n",
    "        for name, module in model_to_prune.named_modules():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                parameters_to_prune.append((module, 'weight'))\n",
    "        \n",
    "        print(f\"Found {len(parameters_to_prune)} Linear layers to prune\")\n",
    "        \n",
    "        # Apply global unstructured pruning\n",
    "        prune.global_unstructured(\n",
    "            parameters_to_prune,\n",
    "            pruning_method=prune.L1Unstructured,\n",
    "            amount=amount\n",
    "        )\n",
    "        \n",
    "        # Calculate sparsity\n",
    "        total_params = 0\n",
    "        zero_params = 0\n",
    "        for name, module in model_to_prune.named_modules():\n",
    "            if isinstance(module, torch.nn.Linear) and hasattr(module, 'weight_mask'):\n",
    "                total_params += module.weight.numel()\n",
    "                zero_params += (module.weight_mask == 0).sum().item()\n",
    "        \n",
    "        actual_sparsity = zero_params / total_params if total_params > 0 else 0\n",
    "        print(f\"Actual sparsity: {actual_sparsity:.3f} ({actual_sparsity*100:.1f}%)\")\n",
    "        \n",
    "        # Evaluate accuracy on CPU\n",
    "        acc = evaluate_accuracy_cpu(model_to_prune, test_loader)\n",
    "        \n",
    "        results.append({\n",
    "            'pruning_amount': amount,\n",
    "            'actual_sparsity': actual_sparsity,\n",
    "            'accuracy': acc\n",
    "        })\n",
    "        \n",
    "        print(f\"Accuracy: {acc:.4f}\")\n",
    "        \n",
    "        # Clean up\n",
    "        del model_to_prune\n",
    "    \n",
    "    # Print final results table\n",
    "    print(\"\\nüìà PRUNING RESULTS SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Pruning | Actual  | Accuracy\")\n",
    "    print(\"Amount  | Sparsity|         \")\n",
    "    print(\"-\" * 50)\n",
    "    for result in results:\n",
    "        print(f\"{result['pruning_amount']*100:6.0f}% | {result['actual_sparsity']*100:6.1f}%  | {result['accuracy']:.4f}\")\n",
    "    \n",
    "    # Create plot\n",
    "    print(\"\\nüìä Generating plot...\")\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(results_df['actual_sparsity'] * 100, results_df['accuracy'], 'bo-', linewidth=2, markersize=8)\n",
    "    plt.xlabel('Sparsity (%)', fontsize=12)\n",
    "    plt.ylabel('Accuracy', fontsize=12)\n",
    "    plt.title('Model Pruning: Sparsity vs Accuracy Trade-off', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(range(10, 60, 10))\n",
    "    \n",
    "    # Add accuracy values as annotations\n",
    "    for i, row in results_df.iterrows():\n",
    "        plt.annotate(f'{row[\"accuracy\"]:.3f}', \n",
    "                    (row['actual_sparsity']*100, row['accuracy']), \n",
    "                    textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Pruning analysis complete!\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def evaluate_accuracy_cpu(model, test_loader):\n",
    "    \"\"\"\n",
    "    CPU-only accuracy evaluation\n",
    "    \n",
    "    Args:\n",
    "        model: Model to evaluate\n",
    "        test_loader: DataLoader for evaluation\n",
    "    \n",
    "    Returns:\n",
    "        accuracy: Float accuracy value\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['labels']\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            \n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de6cc4-ec20-4daa-9b07-4a74e5e5e6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# RoBERTa: PT checkpoint + PRUNING (CPU)\n",
    "# ===============================\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "# Backbone + PT checkpoint path\n",
    "roberta_backbone = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"  # 3-class base head\n",
    "roberta_pt_ckpt  = \"checkpoints/roberta_hp_tuning_study/best_model/best_model.pt\"\n",
    "\n",
    "# Tokenizer & loader\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_backbone, use_fast=True)\n",
    "test_dataset = TweetsDataset(test_df, roberta_tokenizer, max_length=256)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Load base arch w/ new 5-class head, then load your fine-tuned PT weights\n",
    "roberta_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    roberta_backbone, num_labels=5, ignore_mismatched_sizes=True\n",
    ")\n",
    "state = torch.load(roberta_pt_ckpt, map_location=DEVICE)\n",
    "roberta_model.load_state_dict(state[\"model_state_dict\"], strict=False)\n",
    "roberta_model.to(DEVICE).eval()\n",
    "\n",
    "# Run CPU pruning sweep\n",
    "pruning_results_roberta_pt = pruning_tradeoff_sweep_cpu(\n",
    "    roberta_model, test_loader, amounts=[0.1, 0.2, 0.3, 0.4, 0.5]\n",
    ")\n",
    "pruning_results_roberta_pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d2f21c-066e-436f-abfb-548ae09ca317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# RoBERTa: HF checkpoint dir + PRUNING (CPU)\n",
    "# ===============================\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "# Your HF checkpoint directory\n",
    "roberta_hf_ckpt = \"roberta_results_HF/trial_7/checkpoint-18000\"\n",
    "\n",
    "# Use tokenizer from backbone (checkpoint dir may not have tokenizer files)\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\", use_fast=True)\n",
    "test_dataset = TweetsDataset(test_df, roberta_tokenizer, max_length=256)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Load model from local HF checkpoint dir\n",
    "roberta_model_hf = AutoModelForSequenceClassification.from_pretrained(\n",
    "    roberta_hf_ckpt, local_files_only=True\n",
    ")\n",
    "# (optional) enforce 5 labels if config differs\n",
    "if roberta_model_hf.config.num_labels != 5:\n",
    "    roberta_model_hf.classifier.out_proj = torch.nn.Linear(\n",
    "        roberta_model_hf.classifier.out_proj.in_features, 5\n",
    "    )\n",
    "    roberta_model_hf.config.num_labels = 5\n",
    "\n",
    "roberta_model_hf.to(DEVICE).eval()\n",
    "\n",
    "# Run CPU pruning sweep\n",
    "pruning_results_roberta_hf = pruning_tradeoff_sweep_cpu(\n",
    "    roberta_model_hf, test_loader, amounts=[0.1, 0.2, 0.3, 0.4, 0.5]\n",
    ")\n",
    "pruning_results_roberta_hf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49067d11-9223-40e8-b989-ff1c5bb3ea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# DeBERTa: HF checkpoint dir + PRUNING (CPU)\n",
    "# ===============================\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "# HF checkpoint directory you trained with Trainer.save_model / checkpoint-*\n",
    "deberta_hf_ckpt = \"deberta_results_HF/deberta_trial_4/checkpoint-4950\"\n",
    "\n",
    "# Use tokenizer from backbone (checkpoint dir often lacks tokenizer files)\n",
    "deberta_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\", use_fast=True)\n",
    "test_dataset = TweetsDataset(test_df, deberta_tokenizer, max_length=256)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Load model weights from local HF checkpoint dir\n",
    "deberta_model_hf = AutoModelForSequenceClassification.from_pretrained(\n",
    "    deberta_hf_ckpt, local_files_only=True   # treat as local path\n",
    ")\n",
    "# (optional) enforce num_labels=5 if your config wasn‚Äôt saved with 5:\n",
    "if deberta_model_hf.config.num_labels != 5:\n",
    "    deberta_model_hf.classifier = torch.nn.Linear(deberta_model_hf.classifier.in_features, 5)\n",
    "    deberta_model_hf.config.num_labels = 5\n",
    "\n",
    "deberta_model_hf.to(DEVICE).eval()\n",
    "\n",
    "# Run CPU pruning sweep\n",
    "pruning_results_deberta_hf = pruning_tradeoff_sweep_cpu(\n",
    "    deberta_model_hf, test_loader, amounts=[0.1, 0.2, 0.3, 0.4, 0.5]\n",
    ")\n",
    "pruning_results_deberta_hf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d125e6f-fec1-4240-b6b0-c472d00b685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# DeBERTa: PT checkpoint + PRUNING (CPU)\n",
    "# ===============================\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "# Backbone + PT checkpoint path\n",
    "deberta_backbone = \"agentlans/deberta-v3-base-tweet-sentiment\"\n",
    "deberta_pt_ckpt  = \"checkpoints/deberta_hp_tuning_study/best_model/best_model.pt\"\n",
    "\n",
    "# Tokenizer & loader (use backbone tokenizer)\n",
    "deberta_tokenizer = AutoTokenizer.from_pretrained(deberta_backbone, use_fast=True)\n",
    "test_dataset = TweetsDataset(test_df, deberta_tokenizer, max_length=256)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Load base arch then your fine-tuned PT weights\n",
    "deberta_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    deberta_backbone, num_labels=5, ignore_mismatched_sizes=True\n",
    ")\n",
    "state = torch.load(deberta_pt_ckpt, map_location=DEVICE)\n",
    "deberta_model.load_state_dict(state[\"model_state_dict\"], strict=False)\n",
    "deberta_model.to(DEVICE).eval()\n",
    "\n",
    "# Run CPU pruning sweep (10‚Äì50%)\n",
    "pruning_results_deberta_pt = pruning_tradeoff_sweep_cpu(\n",
    "    deberta_model, test_loader, amounts=[0.1, 0.2, 0.3, 0.4, 0.5]\n",
    ")\n",
    "pruning_results_deberta_pt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dee4b0",
   "metadata": {},
   "source": [
    "# Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b067d6c-8f42-472c-ae61-676cdcb5ac55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Knowledge distillation function loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# set device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2312548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all checkpoint files\n",
    "print(\"Extracting RoBERTa Trial 7 HuggingFace checkpoint...\")\n",
    "!tar -xvzf /content/drive/MyDrive/Checkpoints/roberta_trial_7_checkpoint_HF.tgz\n",
    "\n",
    "print(\"Extracting DeBERTa Trial 4 HuggingFace checkpoint...\")\n",
    "!tar -xvzf /content/drive/MyDrive/Checkpoints/deberta_trial_4_checkpoint-HF.tgz\n",
    "\n",
    "print(\"All checkpoints extracted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f1b3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "acc_metric = load(\"accuracy\")\n",
    "prec_metric = load(\"precision\")\n",
    "rec_metric = load(\"recall\")\n",
    "f1_metric = load(\"f1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd203966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate size in MB\n",
    "def get_model_size_mb(model):\n",
    "    return sum(p.numel() for p in model.parameters()) * 4 / (1024 ** 2)  # float32 ‚Üí 4 bytes\n",
    "\n",
    "def print_model_stats(name, model):\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    size_mb = num_params * 4 / (1024 ** 2)  # 4 bytes per float32 param\n",
    "    print(f\"{name} model ‚Üí Parameters: {num_params:,} | Size: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ea7527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\":  acc_metric.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"precision\": prec_metric.compute(predictions=preds, references=labels, average=\"weighted\")[\"precision\"],\n",
    "        \"recall\":    rec_metric.compute(predictions=preds, references=labels,  average=\"weighted\")[\"recall\"],\n",
    "        \"f1\":        f1_metric.compute(predictions=preds, references=labels,   average=\"weighted\")[\"f1\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288f506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model=None, temperature=2.0, alpha=0.5, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher = teacher_model\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Ensure teacher and student models are on the same device\n",
    "        if self.teacher is not None and hasattr(self, 'model'):\n",
    "            device = next(self.model.parameters()).device\n",
    "            self.teacher = self.teacher.to(device)\n",
    "            print(f\"Moved teacher model to device: {device}\")\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        # Ensure teacher model is on the same device as student model\n",
    "        device = next(model.parameters()).device\n",
    "        if self.teacher.device != device:\n",
    "            self.teacher = self.teacher.to(device)\n",
    "\n",
    "        # Move inputs to the same device as the model\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "        outputs_student = model(**inputs)\n",
    "        with torch.no_grad():\n",
    "            outputs_teacher = self.teacher(**inputs)\n",
    "        loss_ce = F.cross_entropy(outputs_student.logits, inputs[\"labels\"])\n",
    "        loss_kl = F.kl_div(\n",
    "            F.log_softmax(outputs_student.logits / self.temperature, dim=-1),\n",
    "            F.softmax(outputs_teacher.logits / self.temperature, dim=1),\n",
    "            reduction='batchmean' # Added missing parenthesis and reduction argument\n",
    "        )\n",
    "        loss = self.alpha * loss_ce + (1 - self.alpha) * loss_kl\n",
    "\n",
    "        return (loss, outputs_student) if return_outputs else loss\n",
    "\n",
    "print(\"DistillationTrainer class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94148db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Starting RoBERTa PyTorch -> DistilRoBERTa Distillation ===\n",
    "print(\"=== Starting RoBERTa PyTorch -> DistilRoBERTa Distillation ===\")\n",
    "\n",
    "# Determine the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load your CSV files into pandas DataFrames\n",
    "train_df = pd.read_csv(\"data/train_df.csv\")\n",
    "test_df = pd.read_csv(\"data/test_df.csv\")\n",
    "\n",
    "# Ensure label column is int\n",
    "train_df[\"label\"] = train_df[\"label\"].astype(int)\n",
    "test_df[\"label\"] = test_df[\"label\"].astype(int)\n",
    "\n",
    "# Convert to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Load tokenizer for the student\n",
    "roberta_pt_tokenizer_student = AutoTokenizer.from_pretrained(\"distilroberta-base\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_roberta_pt(batch):\n",
    "    return roberta_pt_tokenizer_student(batch[\"CleanTweet\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "# Tokenize train/test datasets\n",
    "tokenized_train = train_dataset.map(tokenize_roberta_pt, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_roberta_pt, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "tokenized_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# Load student model\n",
    "roberta_pt_student = AutoModelForSequenceClassification.from_pretrained(\"distilroberta-base\", num_labels=5).to(device)\n",
    "\n",
    "# Move teacher to device (assumes you've already loaded it as `roberta_pt_teacher`)\n",
    "roberta_pt_teacher = roberta_pt_teacher.to(device)\n",
    "\n",
    "print(f\"Teacher model device: {next(roberta_pt_teacher.parameters()).device}\")\n",
    "print(f\"Student model device: {next(roberta_pt_student.parameters()).device}\")\n",
    "\n",
    "\n",
    "\n",
    "# Training arguments\n",
    "args_roberta_pt = TrainingArguments(\n",
    "    output_dir=\"./results_roberta_pt_distill\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"./logs_roberta_pt\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "# Initialize the distillation trainer\n",
    "trainer_roberta_pt_distill = DistillationTrainer(\n",
    "    model=roberta_pt_student,\n",
    "    teacher_model=roberta_pt_teacher,\n",
    "    args=args_roberta_pt,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the student model\n",
    "trainer_roberta_pt_distill.train()\n",
    "\n",
    "# Print model sizes\n",
    "teacher_params = sum(p.numel() for p in roberta_pt_teacher.parameters())\n",
    "student_params = sum(p.numel() for p in roberta_pt_student.parameters())\n",
    "print(\"\\nRoBERTa PyTorch distillation complete!\")\n",
    "print(f\"Teacher model parameters: {teacher_params:,} ({get_model_size_mb(roberta_pt_teacher):.2f} MB)\")\n",
    "print(f\"Student model parameters: {student_params:,} ({get_model_size_mb(roberta_pt_student):.2f} MB)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9adf670-1f55-48b5-965b-3f07d298f164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Distilling from DeBERTa HF (local checkpoint)\n",
      "\n",
      "üéì Starting knowledge distillation with teacher: DeBERTa_HF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28800/28800 [00:03<00:00, 8326.08 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12343/12343 [00:01<00:00, 9105.13 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training student model with DeBERTa_HF teacher...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4500' max='4500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4500/4500 38:26, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.093800</td>\n",
       "      <td>0.932130</td>\n",
       "      <td>0.333873</td>\n",
       "      <td>0.304301</td>\n",
       "      <td>0.805102</td>\n",
       "      <td>0.333873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.905000</td>\n",
       "      <td>0.897950</td>\n",
       "      <td>0.340679</td>\n",
       "      <td>0.335706</td>\n",
       "      <td>0.787899</td>\n",
       "      <td>0.340679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.849200</td>\n",
       "      <td>0.874573</td>\n",
       "      <td>0.475006</td>\n",
       "      <td>0.503715</td>\n",
       "      <td>0.813244</td>\n",
       "      <td>0.475006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.815700</td>\n",
       "      <td>0.867868</td>\n",
       "      <td>0.518756</td>\n",
       "      <td>0.556934</td>\n",
       "      <td>0.809945</td>\n",
       "      <td>0.518756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.799300</td>\n",
       "      <td>0.868927</td>\n",
       "      <td>0.513084</td>\n",
       "      <td>0.548842</td>\n",
       "      <td>0.808302</td>\n",
       "      <td>0.513084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating distilled model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='386' max='386' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [386/386 01:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for DeBERTa_HF:\n",
      "  eval_loss: 0.8689\n",
      "  eval_accuracy: 0.5131\n",
      "  eval_f1: 0.5488\n",
      "  eval_precision: 0.8083\n",
      "  eval_recall: 0.5131\n",
      "  eval_runtime: 102.7847\n",
      "  eval_samples_per_second: 120.0860\n",
      "  eval_steps_per_second: 3.7550\n",
      "üíæ Results saved:\n",
      "  - JSON: ./distillation_results/DeBERTa_HF_results.json\n",
      "  - CSV: ./distillation_results/DeBERTa_HF_summary.csv\n",
      "  - Text: ./distillation_results/DeBERTa_HF_metrics.txt\n",
      "‚úÖ Completed distillation with DeBERTa HF model\n"
     ]
    }
   ],
   "source": [
    "# Distillation 2: DeBERTa PyTorch Teacher -> MiniLM Student\n",
    "print(\"=== Starting DeBERTa PyTorch -> MiniLM Distillation ===\")\n",
    "project = \"deberta-pt-distillation-minilm\"\n",
    "\n",
    "# Student model for DeBERTa distillation\n",
    "minilm_ckpt = \"nreimers/MiniLM-L6-H384-uncased\"\n",
    "deberta_pt_student = AutoModelForSequenceClassification.from_pretrained(minilm_ckpt, num_labels=5)\n",
    "\n",
    "# Move both models to the same device\n",
    "deberta_pt_teacher = deberta_pt_teacher.to(device)\n",
    "deberta_pt_student = deberta_pt_student.to(device)\n",
    "\n",
    "# Load tokenizer for the student\n",
    "deberta_pt_tokenizer_student = AutoTokenizer.from_pretrained(minilm_ckpt)\n",
    "\n",
    "# Tokenize train and test DataFrames\n",
    "tokenized_train = deberta_pt_tokenizer_student(\n",
    "    list(train_df[\"CleanTweet\"]),\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "tokenized_test = deberta_pt_tokenizer_student(\n",
    "    list(test_df[\"CleanTweet\"]),\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Create Hugging Face Dataset objects and add labels\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": tokenized_train[\"input_ids\"],\n",
    "    \"attention_mask\": tokenized_train[\"attention_mask\"],\n",
    "    \"labels\": list(train_df[\"label\"])\n",
    "})\n",
    "test_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": tokenized_test[\"input_ids\"],\n",
    "    \"attention_mask\": tokenized_test[\"attention_mask\"],\n",
    "    \"labels\": list(test_df[\"label\"])\n",
    "})\n",
    "\n",
    "# Training arguments\n",
    "args_deberta_pt = TrainingArguments(\n",
    "    output_dir=\"./results_deberta_pt_distill_minilm\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"./logs_deberta_pt_minilm\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"deberta-distill-pt-minilm\"\n",
    ")\n",
    "\n",
    "# Initialize the distillation trainer\n",
    "trainer_deberta_pt_distill = DistillationTrainer(\n",
    "    model=deberta_pt_student,\n",
    "    teacher_model=deberta_pt_teacher,\n",
    "    args=args_deberta_pt,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the student model\n",
    "trainer_deberta_pt_distill.train()\n",
    "\n",
    "# Print summary of model sizes\n",
    "def print_model_stats(name, model):\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    size_mb = num_params * 4 / (1024 ** 2)  # 4 bytes per param (float32)\n",
    "    print(f\"{name} model ‚Üí Parameters: {num_params:,} | Size: {size_mb:.2f} MB\")\n",
    "\n",
    "print(\"\\nDeBERTa PyTorch -> MiniLM distillation complete!\")\n",
    "print_model_stats(\"Teacher (DeBERTa PT)\", deberta_pt_teacher)\n",
    "print_model_stats(\"Student (MiniLM)\", deberta_pt_student)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7ef2f9-0a0e-4246-8616-ab59a677a17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Distilling from: RoBERTa HF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéì Starting knowledge distillation with teacher: RoBERTa_PT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28800/28800 [00:03<00:00, 8114.51 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12343/12343 [00:01<00:00, 7195.86 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Accelerator.__init__() got an unexpected keyword argument 'dispatch_batches'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m teacher_model \u001b[38;5;241m=\u001b[39m teacher_model\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Perform distillation using the function\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m result_deberta \u001b[38;5;241m=\u001b[39m \u001b[43mperform_knowledge_distillation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mteacher_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mteacher_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRoBERTa_PT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m  \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Completed distillation with DeBERTa PT model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 110\u001b[0m, in \u001b[0;36mperform_knowledge_distillation\u001b[0;34m(teacher_model, teacher_name, train_df, eval_df, device, temperature, alpha, num_epochs, batch_size)\u001b[0m\n\u001b[1;32m     96\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     97\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./distilled_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mteacher_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     98\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m     report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m,                        \u001b[38;5;66;03m# Don't report to wandb/tensorboard\u001b[39;00m\n\u001b[1;32m    107\u001b[0m )\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Create trainer\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mDistillationTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstudent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mteacher_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstudent_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Train and evaluate\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining student model with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mteacher_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m teacher...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 60\u001b[0m, in \u001b[0;36mperform_knowledge_distillation.<locals>.DistillationTrainer.__init__\u001b[0;34m(self, teacher_model, temperature, alpha, *args, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, teacher_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mteacher \u001b[38;5;241m=\u001b[39m teacher_model\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mteacher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:373\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_in_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 373\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_accelerator_and_postprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker \u001b[38;5;241m=\u001b[39m TrainerMemoryTracker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mskip_memory_metrics)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:4252\u001b[0m, in \u001b[0;36mTrainer.create_accelerator_and_postprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4249\u001b[0m gradient_accumulation_plugin \u001b[38;5;241m=\u001b[39m GradientAccumulationPlugin(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgrad_acc_kwargs)\n\u001b[1;32m   4251\u001b[0m \u001b[38;5;66;03m# create accelerator object\u001b[39;00m\n\u001b[0;32m-> 4252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator \u001b[38;5;241m=\u001b[39m \u001b[43mAccelerator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeepspeed_plugin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepspeed_plugin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_plugin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_accumulation_plugin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4255\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4256\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4257\u001b[0m \u001b[38;5;66;03m# some Trainer classes need to use `gather` instead of `gather_for_metrics`, thus we store a flag\u001b[39;00m\n\u001b[1;32m   4258\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather_for_metrics\n",
      "\u001b[0;31mTypeError\u001b[0m: Accelerator.__init__() got an unexpected keyword argument 'dispatch_batches'"
     ]
    }
   ],
   "source": [
    "# Distillation 3: RoBERTa HuggingFace Teacher -> DistilRoBERTa Student\n",
    "print(\"=== Starting RoBERTa HuggingFace -> DistilRoBERTa Distillation ===\")\n",
    "\n",
    "# Student model for RoBERTa HF distillation\n",
    "roberta_hf_student = AutoModelForSequenceClassification.from_pretrained(\"distilroberta-base\", num_labels=5)\n",
    "\n",
    "# Move both models to the same device\n",
    "roberta_hf_teacher = roberta_hf_teacher.to(device)\n",
    "roberta_hf_student = roberta_hf_student.to(device)\n",
    "\n",
    "# Tokenize dataset with DistilRoBERTa tokenizer\n",
    "roberta_hf_tokenizer_student = AutoTokenizer.from_pretrained(\"distilroberta-base\")\n",
    "def tokenize_roberta_hf(batch):\n",
    "    return roberta_hf_tokenizer_student(batch[\"CleanTweet\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_roberta_hf, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_roberta_hf, batched=True)\n",
    "\n",
    "tokenized_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "tokenized_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "# Training arguments for RoBERTa HF distillation\n",
    "args_roberta_hf = TrainingArguments(\n",
    "    output_dir=\"./results_roberta_hf_distill\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"./logs_roberta_hf\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "# Create distillation trainer for RoBERTa HF\n",
    "trainer_roberta_hf_distill = DistillationTrainer(\n",
    "    model=roberta_hf_student,\n",
    "    teacher_model=roberta_hf_teacher,\n",
    "    args=args_roberta_hf,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "# Train the student model\n",
    "trainer_roberta_hf_distill.train()\n",
    "print(\"\\nRoBERTa HuggingFace distillation complete!\")\n",
    "print_model_stats(\"Teacher (RoBERTa HF)\", roberta_hf_teacher)\n",
    "print_model_stats(\"Student (DistilRoBERTa)\", roberta_hf_student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a07daa3-45f1-4aa9-9132-4781ec3233f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distillation 4: DeBERTa HuggingFace Teacher -> MiniLM Student\n",
    "print(\"=== Starting DeBERTa HuggingFace -> MiniLM Distillation ===\")\n",
    "\n",
    "# Student model: MiniLM\n",
    "minilm_ckpt = \"nreimers/MiniLM-L6-H384-uncased\"\n",
    "minilm_student = AutoModelForSequenceClassification.from_pretrained(minilm_ckpt, num_labels=5)\n",
    "\n",
    "# Move both models to the same device\n",
    "deberta_hf_teacher = deberta_hf_teacher.to(device)\n",
    "minilm_student = minilm_student.to(device)\n",
    "\n",
    "# Tokenizer for MiniLM\n",
    "minilm_tokenizer = AutoTokenizer.from_pretrained(minilm_ckpt)\n",
    "\n",
    "def tokenize_minilm(batch):\n",
    "    return minilm_tokenizer(\n",
    "        batch[\"CleanTweet\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "# Convert pandas to HF datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset  = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Tokenize\n",
    "tokenized_train = train_dataset.map(tokenize_minilm, batched=True)\n",
    "tokenized_test  = test_dataset.map(tokenize_minilm, batched=True)\n",
    "\n",
    "# Format for PyTorch\n",
    "tokenized_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "tokenized_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# Training arguments\n",
    "args_deberta_hf = TrainingArguments(\n",
    "    output_dir=\"./results_deberta_hf_distill_minilm\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"./logs_deberta_hf_minilm\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "# Distillation trainer\n",
    "trainer_deberta_hf_distill = DistillationTrainer(\n",
    "    model=minilm_student,\n",
    "    teacher_model=deberta_hf_teacher,\n",
    "    args=args_deberta_hf,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer_deberta_hf_distill.train()\n",
    "print(\"\\nDeBERTa HuggingFace -> MiniLM distillation complete!\")\n",
    "print_model_stats(\"Teacher (DeBERTa HF)\", deberta_hf_teacher)\n",
    "print_model_stats(\"Student (MiniLM)\", minilm_student)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b2ed69-00dd-4293-9583-69f62e727d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison and Summary for All 4 Distillations\n",
    "print(\"=== Comprehensive Model Size Comparison ===\")\n",
    "\n",
    "# Calculate model sizes for all teacher-student pairs\n",
    "\n",
    "# 1. RoBERTa PyTorch\n",
    "roberta_pt_teacher_size = sum(p.numel() for p in roberta_pt_teacher.parameters())\n",
    "roberta_pt_student_size = sum(p.numel() for p in roberta_pt_student.parameters())\n",
    "\n",
    "# 2. DeBERTa PyTorch\n",
    "deberta_pt_teacher_size = sum(p.numel() for p in deberta_pt_teacher.parameters())\n",
    "deberta_pt_student_size = sum(p.numel() for p in deberta_pt_student.parameters())\n",
    "\n",
    "# 3. RoBERTa HuggingFace\n",
    "roberta_hf_teacher_size = sum(p.numel() for p in roberta_hf_teacher.parameters())\n",
    "roberta_hf_student_size = sum(p.numel() for p in roberta_hf_student.parameters())\n",
    "\n",
    "# 4. DeBERTa HuggingFace\n",
    "deberta_hf_teacher_size = sum(p.numel() for p in deberta_hf_teacher.parameters())\n",
    "deberta_hf_student_size = sum(p.numel() for p in deberta_hf_student.parameters())\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"1. RoBERTa PyTorch Teacher -> DistilRoBERTa Student\")\n",
    "print(f\"   Teacher size: {roberta_pt_teacher_size:,} parameters\")\n",
    "print(f\"   Student size: {roberta_pt_student_size:,} parameters\")\n",
    "print(f\"   Compression ratio: {roberta_pt_teacher_size/roberta_pt_student_size:.2f}x\")\n",
    "\n",
    "print(\"\\n2. DeBERTa PyTorch Teacher -> MiniLM  Student\")\n",
    "print(f\"   Teacher size: {deberta_pt_teacher_size:,} parameters\")\n",
    "print(f\"   Student size: {deberta_pt_student_size:,} parameters\")\n",
    "print(f\"   Compression ratio: {deberta_pt_teacher_size/deberta_pt_student_size:.2f}x\")\n",
    "\n",
    "print(\"\\n3. RoBERTa HuggingFace Teacher -> DistilRoBERTa Student\")\n",
    "print(f\"   Teacher size: {roberta_hf_teacher_size:,} parameters\")\n",
    "print(f\"   Student size: {roberta_hf_student_size:,} parameters\")\n",
    "print(f\"   Compression ratio: {roberta_hf_teacher_size/roberta_hf_student_size:.2f}x\")\n",
    "\n",
    "print(\"\\n4. DeBERTa HuggingFace Teacher -> MiniLM  Student\")\n",
    "print(f\"   Teacher size: {deberta_hf_teacher_size:,} parameters\")\n",
    "print(f\"   Student size: {deberta_hf_student_size:,} parameters\")\n",
    "print(f\"   Compression ratio: {deberta_hf_teacher_size/deberta_hf_student_size:.2f}x\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save all distilled models\n",
    "print(\"\\n=== Saving All Distilled Models ===\")\n",
    "\n",
    "# Save RoBERTa PyTorch distilled model\n",
    "roberta_pt_student.save_pretrained(\"./distilled_roberta_pt_student\")\n",
    "roberta_pt_tokenizer_student.save_pretrained(\"./distilled_roberta_pt_student\")\n",
    "\n",
    "# Save DeBERTa PyTorch distilled model\n",
    "deberta_pt_student.save_pretrained(\"./distilled_deberta_pt_student\")\n",
    "deberta_pt_tokenizer_student.save_pretrained(\"./distilled_deberta_pt_student\")\n",
    "\n",
    "# Save RoBERTa HuggingFace distilled model\n",
    "roberta_hf_student.save_pretrained(\"./distilled_roberta_hf_student\")\n",
    "roberta_hf_tokenizer_student.save_pretrained(\"./distilled_roberta_hf_student\")\n",
    "\n",
    "# Save DeBERTa HuggingFace distilled model\n",
    "deberta_hf_student.save_pretrained(\"./distilled_deberta_hf_student\")\n",
    "deberta_hf_tokenizer_student.save_pretrained(\"./distilled_deberta_hf_student\")\n",
    "\n",
    "print(\"All 4 distilled models saved successfully!\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n=== Summary Statistics ===\")\n",
    "total_teacher_params = roberta_pt_teacher_size + deberta_pt_teacher_size + roberta_hf_teacher_size + deberta_hf_teacher_size\n",
    "total_student_params = roberta_pt_student_size + deberta_pt_student_size + roberta_hf_student_size + deberta_hf_student_size\n",
    "overall_compression = total_teacher_params / total_student_params\n",
    "\n",
    "print(f\"Total teacher parameters: {total_teacher_params:,}\")\n",
    "print(f\"Total student parameters: {total_student_params:,}\")\n",
    "print(f\"Overall compression ratio: {overall_compression:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5613ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
