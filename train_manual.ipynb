{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1de9b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy matplotlib seaborn wordcloud scikit-learn torch torchvision torchaudio transformers==4.39.3 safetensors==0.4.2 datasets optuna wandb sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b74641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, RobertaForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import wandb\n",
    "os.environ[\"WANDB_MODE\"] = \"online\"\n",
    "print(\"All imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d996ba-0f4c-4fdc-a417-7dac4e154944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.39.3 in /opt/conda/lib/python3.10/site-packages (4.39.3)\n",
      "Requirement already satisfied: safetensors==0.4.2 in /opt/conda/lib/python3.10/site-packages (0.4.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.3) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.3) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.3) (1.24.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.3) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.3) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.3) (2025.7.34)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.3) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.3) (0.15.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.3) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.3) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.3) (4.11.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.3) (1.1.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.3) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.3) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.3) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.3) (2024.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m‚úÖ torch: 2.3.1\n",
      "‚úÖ transformers: 4.39.3\n",
      "‚úÖ safetensors: 0.4.2\n"
     ]
    }
   ],
   "source": [
    "# Install only missing packages for Docker environment\n",
    "!pip install transformers==4.39.3 safetensors==0.4.2 && \\\n",
    "python -c \"import torch; print('torch:', torch.__version__)\" && \\\n",
    "python -c \"import transformers; print('transformers:', transformers.__version__)\" && \\\n",
    "python -c \"import safetensors; print('safetensors:', safetensors.__version__)\"\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Label mappings\n",
    "LABEL2ID = {\n",
    "    \"Extremely Negative\": 0,\n",
    "    \"Negative\": 1,\n",
    "    \"Neutral\": 2,\n",
    "    \"Positive\": 3,\n",
    "    \"Extremely Positive\": 4\n",
    "}\n",
    "ID2LABEL = {v: k for k, v in LABEL2ID.items()}\n",
    "\n",
    "# Checkpoint configuration\n",
    "STUDY_NAME = \"roberta_hp_tuning_study\"\n",
    "CHECKPOINT_DIR = f\"checkpoints/{STUDY_NAME}\"\n",
    "\n",
    "# Weights & Biases configuration\n",
    "WANDB_PROJECT = \"Roberta hp tunning fixed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e82320d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, RobertaForSequenceClassification\n",
    "import optuna\n",
    "import wandb\n",
    "os.environ[\"WANDB_MODE\"] = \"online\"\n",
    "print(\"All imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df5c0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ead0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Using device: cuda\n",
      "‚öôÔ∏è Configuration loaded:\n",
      "  Model: cardiffnlp/twitter-roberta-base-sentiment-latest\n",
      "  Device: cuda\n",
      "  Labels: 5 classes\n",
      "  Checkpoint dir: checkpoints/roberta_hp_tuning_study\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "MODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "NUM_LABELS = 5\n",
    "MAX_LENGTH = 256\n",
    "EPOCHS = 20\n",
    "N_TRIALS = 10\n",
    "LABEL2ID = {\n",
    "    \"Extremely Negative\": 0,\n",
    "    \"Negative\": 1,\n",
    "    \"Neutral\": 2,\n",
    "    \"Positive\": 3,\n",
    "    \"Extremely Positive\": 4\n",
    "}\n",
    "ID2LABEL = {v: k for k, v in LABEL2ID.items()}\n",
    "STUDY_NAME = \"roberta_hp_tuning_study\"\n",
    "CHECKPOINT_DIR = f\"checkpoints/{STUDY_NAME}\"\n",
    "WANDB_PROJECT = \"Roberta hp tunning fixed\"\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Labels: {len(LABEL2ID)} classes\")\n",
    "print(f\"  Checkpoint dir: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438e4dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>CleanTweet</th>\n",
       "      <th>Text_Length</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>unknown</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>and and</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>unknown</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>unknown</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>unknown</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>unknown</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>Me, ready to go at supermarket during the COVI...</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41138</th>\n",
       "      <td>44951</td>\n",
       "      <td>89903</td>\n",
       "      <td>usa</td>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>Airline pilots offering to stock supermarket s...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Airline pilots offering to stock supermarket s...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41139</th>\n",
       "      <td>44952</td>\n",
       "      <td>89904</td>\n",
       "      <td>unknown</td>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>Response to complaint not provided citing COVI...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>Response to complaint not provided citing COVI...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41140</th>\n",
       "      <td>44953</td>\n",
       "      <td>89905</td>\n",
       "      <td>unknown</td>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>You know it√Ç¬ís getting tough when @KameronWild...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>You know it's getting tough when is rationing ...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41141</th>\n",
       "      <td>44954</td>\n",
       "      <td>89906</td>\n",
       "      <td>unknown</td>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>Is it wrong that the smell of hand sanitizer i...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Is it wrong that the smell of hand sanitizer i...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41142</th>\n",
       "      <td>44955</td>\n",
       "      <td>89907</td>\n",
       "      <td>unknown</td>\n",
       "      <td>14-04-2020</td>\n",
       "      <td>@TartiiCat Well new/used Rift S are going for ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Well new/used Rift S are going for $700.00 on ...</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41143 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       UserName  ScreenName Location     TweetAt  \\\n",
       "0          3799       48751  unknown  16-03-2020   \n",
       "1          3800       48752  unknown  16-03-2020   \n",
       "2          3801       48753  unknown  16-03-2020   \n",
       "3          3802       48754  unknown  16-03-2020   \n",
       "4          3803       48755  unknown  16-03-2020   \n",
       "...         ...         ...      ...         ...   \n",
       "41138     44951       89903      usa  14-04-2020   \n",
       "41139     44952       89904  unknown  14-04-2020   \n",
       "41140     44953       89905  unknown  14-04-2020   \n",
       "41141     44954       89906  unknown  14-04-2020   \n",
       "41142     44955       89907  unknown  14-04-2020   \n",
       "\n",
       "                                           OriginalTweet           Sentiment  \\\n",
       "0      @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral   \n",
       "1      advice Talk to your neighbours family to excha...            Positive   \n",
       "2      Coronavirus Australia: Woolworths to give elde...            Positive   \n",
       "3      My food stock is not the only one which is emp...            Positive   \n",
       "4      Me, ready to go at supermarket during the #COV...  Extremely Negative   \n",
       "...                                                  ...                 ...   \n",
       "41138  Airline pilots offering to stock supermarket s...             Neutral   \n",
       "41139  Response to complaint not provided citing COVI...  Extremely Negative   \n",
       "41140  You know it√Ç¬ís getting tough when @KameronWild...            Positive   \n",
       "41141  Is it wrong that the smell of hand sanitizer i...             Neutral   \n",
       "41142  @TartiiCat Well new/used Rift S are going for ...            Negative   \n",
       "\n",
       "                                              CleanTweet  Text_Length  \\\n",
       "0                                                and and            2   \n",
       "1      advice Talk to your neighbours family to excha...           38   \n",
       "2      Coronavirus Australia: Woolworths to give elde...           13   \n",
       "3      My food stock is not the only one which is emp...           41   \n",
       "4      Me, ready to go at supermarket during the COVI...           39   \n",
       "...                                                  ...          ...   \n",
       "41138  Airline pilots offering to stock supermarket s...           11   \n",
       "41139  Response to complaint not provided citing COVI...           23   \n",
       "41140  You know it's getting tough when is rationing ...           16   \n",
       "41141  Is it wrong that the smell of hand sanitizer i...           18   \n",
       "41142  Well new/used Rift S are going for $700.00 on ...           45   \n",
       "\n",
       "       text_length  \n",
       "0                2  \n",
       "1               38  \n",
       "2               13  \n",
       "3               41  \n",
       "4               39  \n",
       "...            ...  \n",
       "41138           11  \n",
       "41139           23  \n",
       "41140           16  \n",
       "41141           18  \n",
       "41142           45  \n",
       "\n",
       "[41143 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('clean_tweets.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d7a73c-dbb6-4319-ac3e-9d966f859543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading and preprocessing data...\n",
      "‚úÖ Loaded 41143 samples from clean_tweets.csv\n",
      "üìà Data split completed:\n",
      "  Train: 28800 samples\n",
      "  Validation: 12343 samples\n",
      "  Label distribution in train:\n",
      "    {0: 3837, 1: 6941, 2: 5391, 3: 7994, 4: 4637}\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "# ==========================================\n",
    "\n",
    "def load_and_split_data(data_path='clean_tweets.csv', test_size=0.3, random_state=42):\n",
    "    \"\"\"\n",
    "    Load data and split into train/validation sets\n",
    "    \"\"\"\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Loaded {len(df)} samples from {data_path}\")\n",
    "    \n",
    "    # Split data\n",
    "    train_df, eval_df = train_test_split(\n",
    "        df, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state, \n",
    "        stratify=df['Sentiment']\n",
    "    )\n",
    "    \n",
    "    # Apply label mapping\n",
    "    train_df['label'] = train_df['Sentiment'].map(LABEL2ID)\n",
    "    eval_df['label'] = eval_df['Sentiment'].map(LABEL2ID)\n",
    "    \n",
    "    # Keep only required columns\n",
    "    train_df = train_df[['CleanTweet', 'label']]\n",
    "    eval_df = eval_df[['CleanTweet', 'label']]\n",
    "    \n",
    "    print(f\"Data split completed:\")\n",
    "    print(f\"  Train: {len(train_df)} samples\")\n",
    "    print(f\"  Validation: {len(eval_df)} samples\")\n",
    "    print(f\"  Label distribution in train:\")\n",
    "    print(f\"    {train_df['label'].value_counts().sort_index().to_dict()}\")\n",
    "    \n",
    "    return train_df, eval_df\n",
    "\n",
    "# Load and split data\n",
    "train_df, eval_df = load_and_split_data()\n",
    "\n",
    "# Utility functions\n",
    "def early_stop_check(patience, best_val_accuracy, best_val_accuracy_epoch, current_val_accuracy, current_val_accuracy_epoch):\n",
    "    early_stop_flag = False\n",
    "    if current_val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = current_val_accuracy\n",
    "        best_val_accuracy_epoch = current_val_accuracy_epoch\n",
    "    else:\n",
    "        if current_val_accuracy_epoch - best_val_accuracy_epoch > patience:\n",
    "            early_stop_flag = True\n",
    "    return best_val_accuracy, best_val_accuracy_epoch, early_stop_flag\n",
    "\n",
    "def save_trial_checkpoint(trial, best_model_state, best_val_accuracy, model_name):\n",
    "    checkpoint_dir = f\"{CHECKPOINT_DIR}/trial_{trial.number}\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_data = {\n",
    "        'model_state_dict': best_model_state,\n",
    "        'trial_number': trial.number,\n",
    "        'best_val_accuracy': best_val_accuracy,\n",
    "        'hyperparameters': {\n",
    "            'learning_rate': trial.params.get('learning_rate'),\n",
    "            'weight_decay': trial.params.get('weight_decay'),\n",
    "            'patience': trial.params.get('patience'),\n",
    "            'batch_size': trial.params.get('batch_size'),\n",
    "            'num_layers': trial.params.get('num_layers')\n",
    "        },\n",
    "        'model_name': model_name,\n",
    "        'num_labels': NUM_LABELS,\n",
    "        'timestamp': time.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    }\n",
    "    checkpoint_path = f\"{checkpoint_dir}/model_checkpoint.pt\"\n",
    "    torch.save(checkpoint_data, checkpoint_path)\n",
    "    print(f\"Trial {trial.number}: Checkpoint saved to {checkpoint_path} (Accuracy: {best_val_accuracy:.4f})\")\n",
    "    return checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1d3768-9d26-4879-a3a5-0176ac163b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Datasets saved to data/ folder:\n",
      "  - train_df.csv: 28800 samples\n",
      "  - eval_df.csv: 12343 samples\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# SAVE DATA FOR REPRODUCIBILITY\n",
    "# ==========================================\n",
    "\n",
    "def save_datasets(train_df, eval_df, data_dir='data'):\n",
    "    \"\"\"\n",
    "    Save train and validation datasets to CSV files\n",
    "    \"\"\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    train_df.to_csv(f'{data_dir}/train_df.csv', index=False)\n",
    "    eval_df.to_csv(f'{data_dir}/eval_df.csv', index=False)\n",
    "    \n",
    "    print(f\" Datasets saved to {data_dir}/ folder:\")\n",
    "    print(f\"  - train_df.csv: {len(train_df)} samples\")\n",
    "    print(f\"  - eval_df.csv: {len(eval_df)} samples\")\n",
    "\n",
    "# Save datasets\n",
    "save_datasets(train_df, eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dec3fc9-f681-4a40-9c0a-ebbbf20c8c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Initializing tokenizer and model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizer loaded: cardiffnlp/twitter-roberta-base-sentiment-latest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded: cardiffnlp/twitter-roberta-base-sentiment-latest\n",
      "üìä Model parameters: 124,649,477\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# MODEL AND TOKENIZER INITIALIZATION\n",
    "# ==========================================\n",
    "\n",
    "def initialize_tokenizer_and_model():\n",
    "    \"\"\"\n",
    "    Initialize tokenizer and model for the training\n",
    "    \"\"\"\n",
    "    print(\"Initializing tokenizer and model...\")\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "    print(f\"‚úÖ Tokenizer loaded: {MODEL_NAME}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, \n",
    "        num_labels=NUM_LABELS,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(device)\n",
    "    print(f\"Model loaded: {MODEL_NAME}\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    return tokenizer, model\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer, model = initialize_tokenizer_and_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f40600a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training function defined\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# TRAINING FUNCTIONS\n",
    "# ==========================================\n",
    "\n",
    "def train_model_with_hyperparams(model, train_loader, val_loader, optimizer, \n",
    "                                criterion, epochs, patience, trial):\n",
    "    print(f\"Starting training for trial {trial.number}\")\n",
    "    \n",
    "    best_val_accuracy = 0.0\n",
    "    best_val_accuracy_epoch = 0\n",
    "    early_stop_flag = False\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # =============== TRAINING PHASE ===============\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        total_train_samples = 0\n",
    "        correct_train_predictions = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            # Move data to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            train_loss += loss.item() * input_ids.size(0)\n",
    "            total_train_samples += input_ids.size(0)\n",
    "            correct_train_predictions += (logits.argmax(dim=1) == labels).sum().item()\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        train_loss /= total_train_samples\n",
    "        train_accuracy = correct_train_predictions / total_train_samples\n",
    "        \n",
    "        # =============== VALIDATION PHASE ===============\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        total_val_samples = 0\n",
    "        correct_val_predictions = 0\n",
    "        all_val_labels = []\n",
    "        all_val_preds = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                # Move data to device\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "                # Accumulate metrics\n",
    "                val_loss += loss.item() * input_ids.size(0)\n",
    "                total_val_samples += input_ids.size(0)\n",
    "                correct_val_predictions += (logits.argmax(dim=1) == labels).sum().item()\n",
    "                all_val_labels.extend(labels.cpu().numpy())\n",
    "                all_val_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        val_loss /= total_val_samples\n",
    "        val_accuracy = correct_val_predictions / total_val_samples\n",
    "        val_precision = precision_score(all_val_labels, all_val_preds, average='weighted', zero_division=0)\n",
    "        val_recall = recall_score(all_val_labels, all_val_preds, average='weighted', zero_division=0)\n",
    "        val_f1 = f1_score(all_val_labels, all_val_preds, average='weighted', zero_division=0)\n",
    "        \n",
    "        # =============== EARLY STOPPING CHECK ===============\n",
    "        best_val_accuracy, best_val_accuracy_epoch, early_stop_flag = early_stop_check(\n",
    "            patience, best_val_accuracy, best_val_accuracy_epoch, val_accuracy, epoch\n",
    "        )\n",
    "        \n",
    "        # Save best model state\n",
    "        if val_accuracy == best_val_accuracy:\n",
    "            best_model_state = model.state_dict()\n",
    "        \n",
    "        # =============== LOGGING ===============\n",
    "        metrics = {\n",
    "            \"Epoch\": epoch,\n",
    "            \"Train Loss\": train_loss,\n",
    "            \"Train Accuracy\": train_accuracy,\n",
    "            \"Validation Loss\": val_loss,\n",
    "            \"Validation Accuracy\": val_accuracy,\n",
    "            \"Validation Precision\": val_precision,\n",
    "            \"Validation Recall\": val_recall,\n",
    "            \"Validation F1\": val_f1\n",
    "        }\n",
    "        wandb.log(metrics)\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % 5 == 0 or early_stop_flag:\n",
    "            print(f\"  Epoch {epoch}/{epochs}: Val Acc = {val_accuracy:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "        \n",
    "        # Check early stopping\n",
    "        if early_stop_flag:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    # =============== SAVE CHECKPOINT ===============\n",
    "    if best_model_state is not None:\n",
    "        save_trial_checkpoint(trial, best_model_state, best_val_accuracy, MODEL_NAME)\n",
    "    \n",
    "    print(f\"‚úÖ Trial {trial.number} completed: Best Val Acc = {best_val_accuracy:.4f}\")\n",
    "    return best_val_accuracy\n",
    "\n",
    "print(\"‚úÖ Training function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1404b358-da71-4d0f-a356-c1f64c9d759f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 20:24:54,994] A new study created in memory with name: no-name-c7adb456-b48d-4b49-bc8a-d90c19f46178\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ STARTING HYPERPARAMETER OPTIMIZATION\n",
      "============================================================\n",
      "üìä Configuration:\n",
      "  Model: cardiffnlp/twitter-roberta-base-sentiment-latest\n",
      "  Trials: 10\n",
      "  Max epochs per trial: 20\n",
      "  Study name: roberta_hp_tuning_study\n",
      "============================================================\n",
      "\n",
      "üî¨ Starting Trial 0\n",
      "üìã Trial 0 hyperparameters:\n",
      "  learning_rate: 1.341450452085596e-05\n",
      "  weight_decay: 0.0006030527426574259\n",
      "  patience: 3\n",
      "  batch_size: 32\n",
      "  num_layers: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Trainable parameters: 36,033,797 / 124,649,477 (28.9%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.21.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/yagel/ADL/wandb/run-20250816_202457-vvm0n4mk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/vvm0n4mk' target=\"_blank\">trial_0</a></strong> to <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/vvm0n4mk' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/vvm0n4mk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training for trial 0\n",
      "  Epoch 5/20: Val Acc = 0.6958, Val Loss = 0.7864\n",
      "  Epoch 10/20: Val Acc = 0.7073, Val Loss = 0.8616\n",
      "  Epoch 15/20: Val Acc = 0.7091, Val Loss = 0.9966\n",
      "  Epoch 20/20: Val Acc = 0.7301, Val Loss = 1.0323\n",
      "üíæ Trial 0: Checkpoint saved to checkpoints/roberta_hp_tuning_study/trial_0/model_checkpoint.pt (Accuracy: 0.7424)\n",
      "‚úÖ Trial 0 completed: Best Val Acc = 0.7424\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.021 MB of 0.035 MB uploaded\\r'), FloatProgress(value=0.5938697318007663, max=1.0‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>Train Accuracy</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Train Loss</td><td>‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Accuracy</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñá</td></tr><tr><td>Validation F1</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá</td></tr><tr><td>Validation Loss</td><td>‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñà</td></tr><tr><td>Validation Precision</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá</td></tr><tr><td>Validation Recall</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñá</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>20</td></tr><tr><td>Train Accuracy</td><td>0.93726</td></tr><tr><td>Train Loss</td><td>0.17435</td></tr><tr><td>Validation Accuracy</td><td>0.73013</td></tr><tr><td>Validation F1</td><td>0.72856</td></tr><tr><td>Validation Loss</td><td>1.03228</td></tr><tr><td>Validation Precision</td><td>0.73118</td></tr><tr><td>Validation Recall</td><td>0.73013</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_0</strong> at: <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/vvm0n4mk' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/vvm0n4mk</a><br/> View project at: <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250816_202457-vvm0n4mk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 20:59:45,865] Trial 0 finished with value: 0.7423640930081827 and parameters: {'learning_rate': 1.341450452085596e-05, 'weight_decay': 0.0006030527426574259, 'patience': 3, 'batch_size': 32, 'num_layers': 5}. Best is trial 0 with value: 0.7423640930081827.\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¨ Starting Trial 1\n",
      "üìã Trial 1 hyperparameters:\n",
      "  learning_rate: 3.461221145921371e-05\n",
      "  weight_decay: 0.001665931642592584\n",
      "  patience: 3\n",
      "  batch_size: 64\n",
      "  num_layers: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Trainable parameters: 28,945,925 / 124,649,477 (23.2%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.21.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/yagel/ADL/wandb/run-20250816_205948-2t1wdmej</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/2t1wdmej' target=\"_blank\">trial_1</a></strong> to <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/2t1wdmej' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/2t1wdmej</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training for trial 1\n",
      "  Epoch 5/20: Val Acc = 0.6628, Val Loss = 0.8260\n",
      "  Epoch 10/20: Val Acc = 0.6888, Val Loss = 0.7946\n",
      "  Epoch 11/20: Val Acc = 0.7087, Val Loss = 0.7847\n",
      "Early stopping triggered at epoch 11\n",
      "üíæ Trial 1: Checkpoint saved to checkpoints/roberta_hp_tuning_study/trial_1/model_checkpoint.pt (Accuracy: 0.7100)\n",
      "‚úÖ Trial 1 completed: Best Val Acc = 0.7100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.021 MB of 0.021 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà</td></tr><tr><td>Train Accuracy</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>Train Loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Accuracy</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñà</td></tr><tr><td>Validation F1</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñà</td></tr><tr><td>Validation Loss</td><td>‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>Validation Precision</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñà</td></tr><tr><td>Validation Recall</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>11</td></tr><tr><td>Train Accuracy</td><td>0.78969</td></tr><tr><td>Train Loss</td><td>0.54713</td></tr><tr><td>Validation Accuracy</td><td>0.70866</td></tr><tr><td>Validation F1</td><td>0.70387</td></tr><tr><td>Validation Loss</td><td>0.78473</td></tr><tr><td>Validation Precision</td><td>0.71207</td></tr><tr><td>Validation Recall</td><td>0.70866</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_1</strong> at: <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/2t1wdmej' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/2t1wdmej</a><br/> View project at: <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250816_205948-2t1wdmej/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 21:16:23,046] Trial 1 finished with value: 0.709957060682168 and parameters: {'learning_rate': 3.461221145921371e-05, 'weight_decay': 0.001665931642592584, 'patience': 3, 'batch_size': 64, 'num_layers': 4}. Best is trial 0 with value: 0.7423640930081827.\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¨ Starting Trial 2\n",
      "üìã Trial 2 hyperparameters:\n",
      "  learning_rate: 2.6200192299177846e-05\n",
      "  weight_decay: 0.0017368653024776744\n",
      "  patience: 3\n",
      "  batch_size: 64\n",
      "  num_layers: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Trainable parameters: 28,945,925 / 124,649,477 (23.2%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.21.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/yagel/ADL/wandb/run-20250816_211626-frsd1fv5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/frsd1fv5' target=\"_blank\">trial_2</a></strong> to <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/frsd1fv5' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/frsd1fv5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training for trial 2\n",
      "  Epoch 5/20: Val Acc = 0.6596, Val Loss = 0.8558\n",
      "  Epoch 10/20: Val Acc = 0.6982, Val Loss = 0.7884\n",
      "  Epoch 15/20: Val Acc = 0.7337, Val Loss = 0.7513\n",
      "  Epoch 19/20: Val Acc = 0.7249, Val Loss = 0.8003\n",
      "Early stopping triggered at epoch 19\n",
      "üíæ Trial 2: Checkpoint saved to checkpoints/roberta_hp_tuning_study/trial_2/model_checkpoint.pt (Accuracy: 0.7337)\n",
      "‚úÖ Trial 2 completed: Best Val Acc = 0.7337\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.021 MB of 0.035 MB uploaded\\r'), FloatProgress(value=0.5802881378753197, max=1.0‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>Train Accuracy</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>Train Loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Accuracy</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñÜ‚ñà</td></tr><tr><td>Validation F1</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñÜ‚ñà</td></tr><tr><td>Validation Loss</td><td>‚ñà‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÇ</td></tr><tr><td>Validation Precision</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà</td></tr><tr><td>Validation Recall</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñÜ‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>19</td></tr><tr><td>Train Accuracy</td><td>0.84514</td></tr><tr><td>Train Loss</td><td>0.41401</td></tr><tr><td>Validation Accuracy</td><td>0.72495</td></tr><tr><td>Validation F1</td><td>0.72389</td></tr><tr><td>Validation Loss</td><td>0.80029</td></tr><tr><td>Validation Precision</td><td>0.72705</td></tr><tr><td>Validation Recall</td><td>0.72495</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_2</strong> at: <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/frsd1fv5' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/frsd1fv5</a><br/> View project at: <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250816_211626-frsd1fv5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 21:44:56,674] Trial 2 finished with value: 0.7336952118609739 and parameters: {'learning_rate': 2.6200192299177846e-05, 'weight_decay': 0.0017368653024776744, 'patience': 3, 'batch_size': 64, 'num_layers': 4}. Best is trial 0 with value: 0.7423640930081827.\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¨ Starting Trial 3\n",
      "üìã Trial 3 hyperparameters:\n",
      "  learning_rate: 1.1074674048910425e-05\n",
      "  weight_decay: 0.0006059325791242597\n",
      "  patience: 2\n",
      "  batch_size: 32\n",
      "  num_layers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Trainable parameters: 7,682,309 / 124,649,477 (6.2%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.21.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/yagel/ADL/wandb/run-20250816_214459-ekb989xv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/ekb989xv' target=\"_blank\">trial_3</a></strong> to <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/ekb989xv' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/ekb989xv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training for trial 3\n",
      "  Epoch 5/20: Val Acc = 0.4990, Val Loss = 1.1717\n",
      "  Epoch 10/20: Val Acc = 0.5286, Val Loss = 1.1041\n",
      "  Epoch 15/20: Val Acc = 0.5543, Val Loss = 1.0536\n",
      "  Epoch 20/20: Val Acc = 0.5643, Val Loss = 1.0337\n",
      "üíæ Trial 3: Checkpoint saved to checkpoints/roberta_hp_tuning_study/trial_3/model_checkpoint.pt (Accuracy: 0.5729)\n",
      "‚úÖ Trial 3 completed: Best Val Acc = 0.5729\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baecb17000dd4c6bbe83c4f38a9f4e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.021 MB of 0.035 MB uploaded\\r'), FloatProgress(value=0.5808373547569622, max=1.0‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>Train Accuracy</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Train Loss</td><td>‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Accuracy</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>Validation F1</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá</td></tr><tr><td>Validation Loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Precision</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá</td></tr><tr><td>Validation Recall</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>20</td></tr><tr><td>Train Accuracy</td><td>0.57615</td></tr><tr><td>Train Loss</td><td>1.00944</td></tr><tr><td>Validation Accuracy</td><td>0.56429</td></tr><tr><td>Validation F1</td><td>0.55957</td></tr><tr><td>Validation Loss</td><td>1.0337</td></tr><tr><td>Validation Precision</td><td>0.56583</td></tr><tr><td>Validation Recall</td><td>0.56429</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_3</strong> at: <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/ekb989xv' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/ekb989xv</a><br/> View project at: <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250816_214459-ekb989xv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:11:17,559] Trial 3 finished with value: 0.5728753139431256 and parameters: {'learning_rate': 1.1074674048910425e-05, 'weight_decay': 0.0006059325791242597, 'patience': 2, 'batch_size': 32, 'num_layers': 1}. Best is trial 0 with value: 0.7423640930081827.\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¨ Starting Trial 4\n",
      "üìã Trial 4 hyperparameters:\n",
      "  learning_rate: 2.149453179041487e-05\n",
      "  weight_decay: 0.0023824177911430495\n",
      "  patience: 2\n",
      "  batch_size: 32\n",
      "  num_layers: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Trainable parameters: 28,945,925 / 124,649,477 (23.2%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.21.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/yagel/ADL/wandb/run-20250816_221120-v72mjwzu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/v72mjwzu' target=\"_blank\">trial_4</a></strong> to <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/v72mjwzu' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/v72mjwzu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training for trial 4\n",
      "  Epoch 5/20: Val Acc = 0.6742, Val Loss = 0.8103\n",
      "  Epoch 10/20: Val Acc = 0.7028, Val Loss = 0.7715\n",
      "  Epoch 15/20: Val Acc = 0.7121, Val Loss = 0.7524\n",
      "  Epoch 20/20: Val Acc = 0.7250, Val Loss = 0.7504\n",
      "üíæ Trial 4: Checkpoint saved to checkpoints/roberta_hp_tuning_study/trial_4/model_checkpoint.pt (Accuracy: 0.7288)\n",
      "‚úÖ Trial 4 completed: Best Val Acc = 0.7288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16991bc5191e4b7ea9a8e54274da2f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.021 MB of 0.035 MB uploaded\\r'), FloatProgress(value=0.5938197756154037, max=1.0‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>Train Accuracy</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Train Loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Accuracy</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà</td></tr><tr><td>Validation F1</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñà‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñà</td></tr><tr><td>Validation Loss</td><td>‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÖ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÇ</td></tr><tr><td>Validation Precision</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà</td></tr><tr><td>Validation Recall</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>20</td></tr><tr><td>Train Accuracy</td><td>0.79802</td></tr><tr><td>Train Loss</td><td>0.53051</td></tr><tr><td>Validation Accuracy</td><td>0.72503</td></tr><tr><td>Validation F1</td><td>0.72616</td></tr><tr><td>Validation Loss</td><td>0.7504</td></tr><tr><td>Validation Precision</td><td>0.73519</td></tr><tr><td>Validation Recall</td><td>0.72503</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_4</strong> at: <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/v72mjwzu' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/v72mjwzu</a><br/> View project at: <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250816_221120-v72mjwzu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:43:57,059] Trial 4 finished with value: 0.7288341570120717 and parameters: {'learning_rate': 2.149453179041487e-05, 'weight_decay': 0.0023824177911430495, 'patience': 2, 'batch_size': 32, 'num_layers': 4}. Best is trial 0 with value: 0.7423640930081827.\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¨ Starting Trial 5\n",
      "üìã Trial 5 hyperparameters:\n",
      "  learning_rate: 1.903050385689178e-05\n",
      "  weight_decay: 0.0012946357923247147\n",
      "  patience: 1\n",
      "  batch_size: 32\n",
      "  num_layers: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Trainable parameters: 21,858,053 / 124,649,477 (17.5%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.21.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/yagel/ADL/wandb/run-20250816_224400-dr8pwbof</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/dr8pwbof' target=\"_blank\">trial_5</a></strong> to <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/dr8pwbof' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/dr8pwbof</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training for trial 5\n",
      "  Epoch 5/20: Val Acc = 0.6300, Val Loss = 0.9005\n",
      "  Epoch 6/20: Val Acc = 0.5943, Val Loss = 0.9749\n",
      "Early stopping triggered at epoch 6\n",
      "üíæ Trial 5: Checkpoint saved to checkpoints/roberta_hp_tuning_study/trial_5/model_checkpoint.pt (Accuracy: 0.6345)\n",
      "‚úÖ Trial 5 completed: Best Val Acc = 0.6345\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03bccd3c60c24cb29f97c9510d18273c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.021 MB of 0.021 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñà</td></tr><tr><td>Train Accuracy</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñà</td></tr><tr><td>Train Loss</td><td>‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>Validation Accuracy</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñà‚ñà‚ñÖ</td></tr><tr><td>Validation F1</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñà‚ñá‚ñÑ</td></tr><tr><td>Validation Loss</td><td>‚ñà‚ñÑ‚ñÑ‚ñÅ‚ñÅ‚ñÑ</td></tr><tr><td>Validation Precision</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñà‚ñá‚ñÜ</td></tr><tr><td>Validation Recall</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñà‚ñà‚ñÖ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>6</td></tr><tr><td>Train Accuracy</td><td>0.66944</td></tr><tr><td>Train Loss</td><td>0.81649</td></tr><tr><td>Validation Accuracy</td><td>0.59434</td></tr><tr><td>Validation F1</td><td>0.58551</td></tr><tr><td>Validation Loss</td><td>0.97494</td></tr><tr><td>Validation Precision</td><td>0.61663</td></tr><tr><td>Validation Recall</td><td>0.59434</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_5</strong> at: <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/dr8pwbof' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/dr8pwbof</a><br/> View project at: <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250816_224400-dr8pwbof/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:53:19,385] Trial 5 finished with value: 0.6345296929433687 and parameters: {'learning_rate': 1.903050385689178e-05, 'weight_decay': 0.0012946357923247147, 'patience': 1, 'batch_size': 32, 'num_layers': 3}. Best is trial 0 with value: 0.7423640930081827.\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¨ Starting Trial 6\n",
      "üìã Trial 6 hyperparameters:\n",
      "  learning_rate: 1.972708524548979e-05\n",
      "  weight_decay: 0.0006305274410348813\n",
      "  patience: 2\n",
      "  batch_size: 64\n",
      "  num_layers: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Trainable parameters: 14,770,181 / 124,649,477 (11.8%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.21.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/yagel/ADL/wandb/run-20250816_225322-lbc7unt5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/lbc7unt5' target=\"_blank\">trial_6</a></strong> to <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/lbc7unt5' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/lbc7unt5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training for trial 6\n",
      "  Epoch 5/20: Val Acc = 0.5794, Val Loss = 1.0075\n",
      "  Epoch 10/20: Val Acc = 0.6114, Val Loss = 0.9453\n",
      "  Epoch 15/20: Val Acc = 0.6204, Val Loss = 0.9430\n",
      "  Epoch 19/20: Val Acc = 0.6302, Val Loss = 0.9421\n",
      "Early stopping triggered at epoch 19\n",
      "üíæ Trial 6: Checkpoint saved to checkpoints/roberta_hp_tuning_study/trial_6/model_checkpoint.pt (Accuracy: 0.6334)\n",
      "‚úÖ Trial 6 completed: Best Val Acc = 0.6334\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b5f26161d5d46f5a1b2a2729f6c57b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.021 MB of 0.021 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>Train Accuracy</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>Train Loss</td><td>‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Accuracy</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>Validation F1</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>Validation Loss</td><td>‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>Validation Precision</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>Validation Recall</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>19</td></tr><tr><td>Train Accuracy</td><td>0.70087</td></tr><tr><td>Train Loss</td><td>0.73873</td></tr><tr><td>Validation Accuracy</td><td>0.63015</td></tr><tr><td>Validation F1</td><td>0.62884</td></tr><tr><td>Validation Loss</td><td>0.94207</td></tr><tr><td>Validation Precision</td><td>0.6349</td></tr><tr><td>Validation Recall</td><td>0.63015</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_6</strong> at: <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/lbc7unt5' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/lbc7unt5</a><br/> View project at: <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250816_225322-lbc7unt5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 23:18:05,930] Trial 6 finished with value: 0.6333954468119581 and parameters: {'learning_rate': 1.972708524548979e-05, 'weight_decay': 0.0006305274410348813, 'patience': 2, 'batch_size': 64, 'num_layers': 2}. Best is trial 0 with value: 0.7423640930081827.\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¨ Starting Trial 7\n",
      "üìã Trial 7 hyperparameters:\n",
      "  learning_rate: 1.3162321355244273e-05\n",
      "  weight_decay: 0.001397614001535171\n",
      "  patience: 4\n",
      "  batch_size: 64\n",
      "  num_layers: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Trainable parameters: 36,033,797 / 124,649,477 (28.9%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.21.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/yagel/ADL/wandb/run-20250816_231809-zm3mlyy0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/zm3mlyy0' target=\"_blank\">trial_7</a></strong> to <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/zm3mlyy0' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/zm3mlyy0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training for trial 7\n",
      "  Epoch 5/20: Val Acc = 0.6686, Val Loss = 0.8244\n",
      "  Epoch 10/20: Val Acc = 0.6725, Val Loss = 0.8649\n",
      "  Epoch 15/20: Val Acc = 0.7220, Val Loss = 0.7946\n",
      "  Epoch 20/20: Val Acc = 0.7190, Val Loss = 0.9100\n",
      "üíæ Trial 7: Checkpoint saved to checkpoints/roberta_hp_tuning_study/trial_7/model_checkpoint.pt (Accuracy: 0.7326)\n",
      "‚úÖ Trial 7 completed: Best Val Acc = 0.7326\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2419d960026a40028233520ff6565ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.021 MB of 0.021 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>Train Accuracy</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Train Loss</td><td>‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Accuracy</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá</td></tr><tr><td>Validation F1</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá</td></tr><tr><td>Validation Loss</td><td>‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ</td></tr><tr><td>Validation Precision</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>Validation Recall</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>20</td></tr><tr><td>Train Accuracy</td><td>0.88444</td></tr><tr><td>Train Loss</td><td>0.31277</td></tr><tr><td>Validation Accuracy</td><td>0.71895</td></tr><tr><td>Validation F1</td><td>0.71911</td></tr><tr><td>Validation Loss</td><td>0.91005</td></tr><tr><td>Validation Precision</td><td>0.72812</td></tr><tr><td>Validation Recall</td><td>0.71895</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_7</strong> at: <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/zm3mlyy0' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/zm3mlyy0</a><br/> View project at: <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250816_231809-zm3mlyy0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 23:50:07,451] Trial 7 finished with value: 0.7326419833103783 and parameters: {'learning_rate': 1.3162321355244273e-05, 'weight_decay': 0.001397614001535171, 'patience': 4, 'batch_size': 64, 'num_layers': 5}. Best is trial 0 with value: 0.7423640930081827.\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¨ Starting Trial 8\n",
      "üìã Trial 8 hyperparameters:\n",
      "  learning_rate: 2.089408867567441e-05\n",
      "  weight_decay: 0.0007438890737486833\n",
      "  patience: 1\n",
      "  batch_size: 32\n",
      "  num_layers: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Trainable parameters: 36,033,797 / 124,649,477 (28.9%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.21.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/yagel/ADL/wandb/run-20250816_235010-yqh3302y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/yqh3302y' target=\"_blank\">trial_8</a></strong> to <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/yqh3302y' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/yqh3302y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training for trial 8\n",
      "  Epoch 5/20: Val Acc = 0.7205, Val Loss = 0.7288\n",
      "  Epoch 10/20: Val Acc = 0.7356, Val Loss = 0.8230\n",
      "Early stopping triggered at epoch 10\n",
      "üíæ Trial 8: Checkpoint saved to checkpoints/roberta_hp_tuning_study/trial_8/model_checkpoint.pt (Accuracy: 0.7419)\n",
      "‚úÖ Trial 8 completed: Best Val Acc = 0.7419\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80092f44ecd64bd1be19c3b8439e4747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.021 MB of 0.021 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà</td></tr><tr><td>Train Accuracy</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>Train Loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>Validation Accuracy</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñà‚ñà‚ñá‚ñà</td></tr><tr><td>Validation F1</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñà‚ñà‚ñá‚ñà</td></tr><tr><td>Validation Loss</td><td>‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÖ</td></tr><tr><td>Validation Precision</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà</td></tr><tr><td>Validation Recall</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñà‚ñà‚ñá‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.8774</td></tr><tr><td>Train Loss</td><td>0.33693</td></tr><tr><td>Validation Accuracy</td><td>0.73564</td></tr><tr><td>Validation F1</td><td>0.73492</td></tr><tr><td>Validation Loss</td><td>0.823</td></tr><tr><td>Validation Precision</td><td>0.74075</td></tr><tr><td>Validation Recall</td><td>0.73564</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_8</strong> at: <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/yqh3302y' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/yqh3302y</a><br/> View project at: <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250816_235010-yqh3302y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-17 00:07:37,522] Trial 8 finished with value: 0.7418779875232926 and parameters: {'learning_rate': 2.089408867567441e-05, 'weight_decay': 0.0007438890737486833, 'patience': 1, 'batch_size': 32, 'num_layers': 5}. Best is trial 0 with value: 0.7423640930081827.\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¨ Starting Trial 9\n",
      "üìã Trial 9 hyperparameters:\n",
      "  learning_rate: 2.593208663848655e-05\n",
      "  weight_decay: 0.0022497231399703607\n",
      "  patience: 4\n",
      "  batch_size: 64\n",
      "  num_layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Trainable parameters: 43,121,669 / 124,649,477 (34.6%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.21.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/yagel/ADL/wandb/run-20250817_000742-2h69i0j5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/2h69i0j5' target=\"_blank\">trial_9</a></strong> to <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/2h69i0j5' target=\"_blank\">https://wandb.ai/yagelalfasi-tau/Roberta%20hp%20tunning%20fixed/runs/2h69i0j5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training for trial 9\n",
      "  Epoch 5/20: Val Acc = 0.7257, Val Loss = 0.7198\n"
     ]
    }
   ],
   "source": [
    "# HYPERPARAMETER OPTIMIZATION\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for hyperparameter optimization\n",
    "    \n",
    "    Args:\n",
    "        trial: Optuna trial object\n",
    "    \n",
    "    Returns:\n",
    "        float: Best validation accuracy for this trial\n",
    "    \"\"\"\n",
    "    print(f\"\\nüî¨ Starting Trial {trial.number}\")\n",
    "    \n",
    "    # =============== HYPERPARAMETER SUGGESTIONS ===============\n",
    "    hyperparams = {\n",
    "        'learning_rate': trial.suggest_float(\"learning_rate\", 1e-5, 3.5e-5, log=True),\n",
    "        'weight_decay': trial.suggest_float(\"weight_decay\", 5e-4, 3e-3, log=True),\n",
    "        'patience': trial.suggest_int(\"patience\", 1, 4),\n",
    "        'batch_size': trial.suggest_categorical(\"batch_size\", [32, 64]),\n",
    "        'num_layers': trial.suggest_int(\"num_layers\", 1, 6)\n",
    "    }\n",
    "    \n",
    "    print(f\"üìã Trial {trial.number} hyperparameters:\")\n",
    "    for key, value in hyperparams.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # =============== DATA LOADERS ===============\n",
    "    train_dataset = TweetsDataset(train_df, tokenizer)\n",
    "    val_dataset = TweetsDataset(eval_df, tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=hyperparams['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=hyperparams['batch_size'], shuffle=False)\n",
    "    \n",
    "    # =============== MODEL SETUP ===============\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_LABELS,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # =============== LAYER FREEZING ===============\n",
    "    # Freeze all roberta layers\n",
    "    for param in model.roberta.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Unfreeze last `num_layers` layers\n",
    "    num_layers_to_unfreeze = hyperparams['num_layers']\n",
    "    for param in model.roberta.encoder.layer[-num_layers_to_unfreeze:].parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    # Always keep classifier trainable\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"üîß Trainable parameters: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.1f}%)\")\n",
    "    \n",
    "    # =============== OPTIMIZER AND LOSS ===============\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=hyperparams['learning_rate'], \n",
    "        weight_decay=hyperparams['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # =============== WEIGHTS & BIASES SETUP ===============\n",
    "    wandb.finish()  # Clean up any previous runs\n",
    "    time.sleep(1)  # Small delay\n",
    "    \n",
    "    wandb.init(\n",
    "        project=WANDB_PROJECT,\n",
    "        config=hyperparams,\n",
    "        name=f\"trial_{trial.number}\",\n",
    "        mode=\"online\",\n",
    "        reinit=True,\n",
    "        settings=wandb.Settings(start_method=\"thread\")\n",
    "    )\n",
    "    \n",
    "    # =============== TRAINING ===============\n",
    "    try:\n",
    "        best_val_accuracy = train_model_with_hyperparams(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            epochs=EPOCHS,\n",
    "            patience=hyperparams['patience'],\n",
    "            trial=trial\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Trial {trial.number} failed: {e}\")\n",
    "        best_val_accuracy = 0.0\n",
    "    finally:\n",
    "        wandb.finish()\n",
    "        # Clean up GPU memory\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return best_val_accuracy\n",
    "\n",
    "def run_hyperparameter_optimization():\n",
    "    \"\"\"\n",
    "    Run the complete hyperparameter optimization process\n",
    "    \"\"\"\n",
    "    print(\"STARTING HYPERPARAMETER OPTIMIZATION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"  Model: {MODEL_NAME}\")\n",
    "    print(f\"  Trials: {N_TRIALS}\")\n",
    "    print(f\"  Max epochs per trial: {EPOCHS}\")\n",
    "    print(f\"  Study name: {STUDY_NAME}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Clean up any previous wandb runs\n",
    "    wandb.finish()\n",
    "    \n",
    "    # Create and run study\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=N_TRIALS)\n",
    "    \n",
    "    return study\n",
    "\n",
    "# =============== RUN OPTIMIZATION ===============\n",
    "study = run_hyperparameter_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf91928b-faa8-49fc-ab88-c72fb5687e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# OPTIMIZATION RESULTS AND BEST MODEL SAVING\n",
    "# ==========================================\n",
    "\n",
    "def save_study_best_model(study):\n",
    "    \"\"\"\n",
    "    Save the best model from the completed study\n",
    "    \n",
    "    Args:\n",
    "        study: Completed Optuna study\n",
    "    \"\"\"\n",
    "    print(\"SAVING STUDY BEST MODEL\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    best_trial_num = study.best_trial.number\n",
    "    best_checkpoint_path = f\"{CHECKPOINT_DIR}/trial_{best_trial_num}/model_checkpoint.pt\"\n",
    "    \n",
    "    if os.path.exists(best_checkpoint_path):\n",
    "        # Load the best trial's checkpoint\n",
    "        best_checkpoint = torch.load(best_checkpoint_path, map_location=device)\n",
    "        \n",
    "        # Create study-level best model directory\n",
    "        study_best_dir = f\"{CHECKPOINT_DIR}/best_model\"\n",
    "        os.makedirs(study_best_dir, exist_ok=True)\n",
    "        \n",
    "        # Add study-level metadata\n",
    "        study_best_checkpoint = {\n",
    "            **best_checkpoint,\n",
    "            'study_name': STUDY_NAME,\n",
    "            'study_best_trial': study.best_trial.number,\n",
    "            'study_best_value': study.best_value,\n",
    "            'total_trials': len(study.trials),\n",
    "            'optimization_completed': time.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "        }\n",
    "        \n",
    "        # Save the study's best model\n",
    "        study_best_path = f\"{study_best_dir}/best_model.pt\"\n",
    "        torch.save(study_best_checkpoint, study_best_path)\n",
    "        \n",
    "        print(f\"Study best model saved:\")\n",
    "        print(f\"Location: {study_best_path}\")\n",
    "        print(f\"Accuracy: {study.best_value:.4f}\")\n",
    "        print(f\"From trial: {study.best_trial.number}\")\n",
    "        \n",
    "        return study_best_path\n",
    "    else:\n",
    "        print(f\"‚ùå Best trial checkpoint not found at {best_checkpoint_path}\")\n",
    "        return None\n",
    "\n",
    "def display_optimization_results(study):\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"OPTIMIZATION COMPLETED\")\n",
    "    \n",
    "    # Basic results\n",
    "    print(f\"Best trial: {study.best_trial.number}\")\n",
    "    print(f\"Best accuracy: {study.best_value:.4f}\")\n",
    "    print(f\"Total trials: {len(study.trials)}\")\n",
    "    print(f\"Completed trials: {len([t for t in study.trials if t.value is not None])}\")\n",
    "    \n",
    "    # Best hyperparameters\n",
    "    print(f\"\\nüìã Best hyperparameters:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        if isinstance(value, float) and value < 0.001:\n",
    "            print(f\"  {key:15s}: {value:.2e}\")\n",
    "        else:\n",
    "            print(f\"  {key:15s}: {value}\")\n",
    "    \n",
    "    # Trial performance summary\n",
    "    trial_values = [trial.value for trial in study.trials if trial.value is not None]\n",
    "    if trial_values:\n",
    "        print(f\"\\nPerformance summary:\")\n",
    "        print(f\"  Best accuracy:    {max(trial_values):.4f}\")\n",
    "        print(f\"  Average accuracy: {np.mean(trial_values):.4f}\")\n",
    "        print(f\"  Std deviation:    {np.std(trial_values):.4f}\")\n",
    "    \n",
    "\n",
    "    \n",
    "    return study.best_value, study.best_params\n",
    "\n",
    "# =============== PROCESS RESULTS ===============\n",
    "best_accuracy, best_params = display_optimization_results(study)\n",
    "best_model_path = save_study_best_model(study)\n",
    "\n",
    "print(f\"\\nOptimization and checkpoint saving completed!\")\n",
    "print(f\"Ready to load model from: {best_model_path}\")\n",
    "print(\"Use the checkpoint loading utilities in the next cell to load your trained model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b261abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# CHECKPOINT LOADING UTILITIES\n",
    "# ==========================================\n",
    "\n",
    "class CheckpointManager:\n",
    "    \"\"\"\n",
    "    Manager class for loading and managing saved model checkpoints\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, study_name=STUDY_NAME):\n",
    "        self.study_name = study_name\n",
    "        self.checkpoints_dir = f\"checkpoints/{study_name}\"\n",
    "    \n",
    "    def load_best_study_model(self):\n",
    "\n",
    "\n",
    "        best_model_path = f\"{self.checkpoints_dir}/best_model/best_model.pt\"\n",
    "        \n",
    "        if not os.path.exists(best_model_path):\n",
    "            print(f\"‚ùå Best model not found at {best_model_path}\")\n",
    "            return None, None\n",
    "        \n",
    "        try:\n",
    "            # Load checkpoint data\n",
    "            checkpoint_data = torch.load(best_model_path, map_location=device)\n",
    "            \n",
    "            # Recreate model with same configuration\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                checkpoint_data['model_name'],\n",
    "                num_labels=checkpoint_data['num_labels'],\n",
    "                ignore_mismatched_sizes=True\n",
    "            ).to(device)\n",
    "            \n",
    "            # Load the saved state\n",
    "            model.load_state_dict(checkpoint_data['model_state_dict'])\n",
    "            \n",
    "            print(f\" Loaded best model from study: {self.study_name}\")\n",
    "            print(f\" Accuracy: {checkpoint_data['best_val_accuracy']:.4f}\")\n",
    "            print(f\" From trial: {checkpoint_data['trial_number']}\")\n",
    "            print(f\" Saved: {checkpoint_data.get('timestamp', 'Unknown')}\")\n",
    "            print(f\" Hyperparameters:\")\n",
    "            for param, value in checkpoint_data['hyperparameters'].items():\n",
    "                print(f\"  {param}: {value}\")\n",
    "            \n",
    "            return model, checkpoint_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading best model: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def load_trial_model(self, trial_number):\n",
    "        \n",
    "        trial_path = f\"{self.checkpoints_dir}/trial_{trial_number}/model_checkpoint.pt\"\n",
    "        \n",
    "        if not os.path.exists(trial_path):\n",
    "            print(f\"‚ùå Trial {trial_number} checkpoint not found at {trial_path}\")\n",
    "            return None, None\n",
    "        \n",
    "        try:\n",
    "            # Load checkpoint data\n",
    "            checkpoint_data = torch.load(trial_path, map_location=device)\n",
    "            \n",
    "            # Recreate model\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                checkpoint_data['model_name'],\n",
    "                num_labels=checkpoint_data['num_labels'],\n",
    "                ignore_mismatched_sizes=True\n",
    "            ).to(device)\n",
    "            \n",
    "            # Load the saved state\n",
    "            model.load_state_dict(checkpoint_data['model_state_dict'])\n",
    "            \n",
    "            print(f\"Loaded trial {trial_number} model\")\n",
    "            print(f\"Accuracy: {checkpoint_data['best_val_accuracy']:.4f}\")\n",
    "            \n",
    "            return model, checkpoint_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading trial {trial_number}: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def list_saved_checkpoints(self):\n",
    "        \"\"\"\n",
    "        List all available checkpoints with their performance\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.checkpoints_dir):\n",
    "            print(f\"‚ùå No checkpoints found for study: {self.study_name}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"üìã SAVED CHECKPOINTS FOR STUDY: {self.study_name}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Check for best model\n",
    "        best_model_path = f\"{self.checkpoints_dir}/best_model/best_model.pt\"\n",
    "        if os.path.exists(best_model_path):\n",
    "            try:\n",
    "                checkpoint_data = torch.load(best_model_path, map_location='cpu')\n",
    "                print(f\"STUDY BEST: Trial {checkpoint_data['trial_number']} - Accuracy: {checkpoint_data['best_val_accuracy']:.4f}\")\n",
    "                print(f\"Location: {best_model_path}\")\n",
    "            except Exception:\n",
    "                print(\"STUDY BEST: Available but corrupted\")\n",
    "        else:\n",
    "            print(\"STUDY BEST: Not available\")\n",
    "        \n",
    "        # List trial checkpoints\n",
    "        trial_data = []\n",
    "        for item in os.listdir(self.checkpoints_dir):\n",
    "            if item.startswith(\"trial_\"):\n",
    "                trial_path = f\"{self.checkpoints_dir}/{item}/model_checkpoint.pt\"\n",
    "                if os.path.exists(trial_path):\n",
    "                    try:\n",
    "                        checkpoint_data = torch.load(trial_path, map_location='cpu')\n",
    "                        trial_data.append({\n",
    "                            'trial': checkpoint_data['trial_number'],\n",
    "                            'accuracy': checkpoint_data['best_val_accuracy'],\n",
    "                            'path': trial_path\n",
    "                        })\n",
    "                    except Exception:\n",
    "                        continue\n",
    "        \n",
    "        # Sort by accuracy\n",
    "        trial_data.sort(key=lambda x: x['accuracy'], reverse=True)\n",
    "        \n",
    "        print(f\"\\nTRIAL CHECKPOINTS ({len(trial_data)} available):\")\n",
    "        print(\"-\" * 60)\n",
    "        for i, trial in enumerate(trial_data):\n",
    "            status = \"ü•á\" if i == 0 else \"ü•à\" if i == 1 else \"ü•â\" if i == 2 else \"üì¶\"\n",
    "            print(f\"{status} Trial {trial['trial']:2d}: Accuracy={trial['accuracy']:.4f}\")\n",
    "\n",
    "# Initialize checkpoint manager\n",
    "checkpoint_manager = CheckpointManager()\n",
    "\n",
    "print(\"üõ†Ô∏è CHECKPOINT MANAGER LOADED\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Available methods:\")\n",
    "print(\"  üì• checkpoint_manager.load_best_study_model()     - Load the best model from study\")\n",
    "print(\"  üì• checkpoint_manager.load_trial_model(trial_num) - Load a specific trial's model\")\n",
    "print(\"  üìã checkpoint_manager.list_saved_checkpoints()    - Show all available checkpoints\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"  model, data = checkpoint_manager.load_best_study_model()\")\n",
    "print(\"  checkpoint_manager.list_saved_checkpoints()\")\n",
    "print(\"  model, data = checkpoint_manager.load_trial_model(5)\")\n",
    "\n",
    "checkpoint_manager.list_saved_checkpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceb2875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DeBERTa test set results to CSV\n",
    "import pandas as pd\n",
    "if 'test_df_deberta' in globals() and 'predicted_label' in test_df_deberta.columns:\n",
    "    test_df_deberta.to_csv('test_set_results_deberta_manual.csv', index=False)\n",
    "    print(\"DeBERTa test set results saved to test_set_results_deberta_manual.csv\")\n",
    "else:\n",
    "    print(\"No DeBERTa test set predictions found to save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2483f76b-f770-48cd-bdcb-fc1245c1e0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# DEBERTA MODEL HYPERPARAMETER OPTIMIZATION - STANDALONE\n",
    "# ==========================================\n",
    "\n",
    "print(\"STARTING STANDALONE DEBERTA MODEL OPTIMIZATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"This cell runs independently and doesn't require RoBERTa completion\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# DeBERTa model configuration\n",
    "DEBERTA_MODEL_NAME = \"agentlans/deberta-v3-base-tweet-sentiment\"\n",
    "DEBERTA_STUDY_NAME = \"deberta_hp_tuning_study\"\n",
    "DEBERTA_CHECKPOINT_DIR = f\"checkpoints/{DEBERTA_STUDY_NAME}\"\n",
    "DEBERTA_WANDB_PROJECT = \"DeBERTa hp tuning fixed\"\n",
    "\n",
    "print(f\"DeBERTa Configuration:\")\n",
    "print(f\"  Model: {DEBERTA_MODEL_NAME}\")\n",
    "print(f\"  Trials: {N_TRIALS}\")\n",
    "print(f\"  Max epochs per trial: {EPOCHS}\")\n",
    "print(f\"  Study name: {DEBERTA_STUDY_NAME}\")\n",
    "print(f\"  Checkpoint dir: {DEBERTA_CHECKPOINT_DIR}\")\n",
    "print(f\"  W&B Project: {DEBERTA_WANDB_PROJECT}\")\n",
    "\n",
    "# Ensure we have the data available\n",
    "print(f\"\\nData availability check:\")\n",
    "try:\n",
    "    print(f\"  Train data: {len(train_df)} samples\")\n",
    "    print(f\"  Validation data: {len(eval_df)} samples\")\n",
    "    data_available = True\n",
    "except NameError:\n",
    "    print(\" Data not found. Loading from saved files...\")\n",
    "    try:\n",
    "        train_df = pd.read_csv('data/train_df.csv')\n",
    "        eval_df = pd.read_csv('data/eval_df.csv')\n",
    "        print(f\" Loaded train data: {len(train_df)} samples\")\n",
    "        print(f\" Loaded validation data: {len(eval_df)} samples\")\n",
    "        data_available = True\n",
    "    except:\n",
    "        print(\"  ‚ùå Could not load data. Please run data preprocessing cells first.\")\n",
    "        data_available = False\n",
    "\n",
    "if data_available:\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    def deberta_objective(trial):\n",
    "\n",
    "        print(f\"\\n Starting DeBERTa Trial {trial.number}\")\n",
    "        \n",
    "        # =============== HYPERPARAMETER SUGGESTIONS ===============\n",
    "        hyperparams = {\n",
    "            'learning_rate': trial.suggest_float(\"learning_rate\", 1e-5, 3.5e-5, log=True),\n",
    "            'weight_decay': trial.suggest_float(\"weight_decay\", 5e-4, 3e-3, log=True),\n",
    "            'patience': trial.suggest_int(\"patience\", 1, 4),\n",
    "            'batch_size': trial.suggest_categorical(\"batch_size\", [32, 64]),  # Smaller batch for DeBERTa\n",
    "            'num_layers': trial.suggest_int(\"num_layers\", 1, 6)\n",
    "        }\n",
    "        \n",
    "        print(f\"DeBERTa Trial {trial.number} hyperparameters:\")\n",
    "        for key, value in hyperparams.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "\n",
    "        deberta_tokenizer = AutoTokenizer.from_pretrained(DEBERTA_MODEL_NAME, use_fast=False)\n",
    "        \n",
    "        train_dataset = TweetsDataset(train_df, deberta_tokenizer)\n",
    "        val_dataset = TweetsDataset(eval_df, deberta_tokenizer)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=hyperparams['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=hyperparams['batch_size'], shuffle=False)\n",
    "        \n",
    "        # =============== MODEL SETUP ===============\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            DEBERTA_MODEL_NAME,\n",
    "            num_labels=NUM_LABELS,\n",
    "            ignore_mismatched_sizes=True\n",
    "        ).to(device)\n",
    "        \n",
    "        # =============== LAYER FREEZING ===============\n",
    "        # Freeze all deberta layers\n",
    "        for param in model.deberta.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Unfreeze last `num_layers` layers\n",
    "        num_layers_to_unfreeze = hyperparams['num_layers']\n",
    "        for param in model.deberta.encoder.layer[-num_layers_to_unfreeze:].parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # Always keep classifier trainable\n",
    "        for param in model.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"Trainable parameters: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.1f}%)\")\n",
    "        \n",
    "        # =============== OPTIMIZER AND LOSS ===============\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(), \n",
    "            lr=hyperparams['learning_rate'], \n",
    "            weight_decay=hyperparams['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # =============== WEIGHTS & BIASES SETUP ===============\n",
    "        wandb.finish()  # Clean up any previous runs\n",
    "        time.sleep(1)  # Small delay\n",
    "        \n",
    "        wandb.init(\n",
    "            project=DEBERTA_WANDB_PROJECT,\n",
    "            config=hyperparams,\n",
    "            name=f\"deberta_trial_{trial.number}\",\n",
    "            mode=\"online\",\n",
    "            reinit=True,\n",
    "            settings=wandb.Settings(start_method=\"thread\")\n",
    "        )\n",
    "        \n",
    "        # =============== TRAINING ===============\n",
    "        try:\n",
    "            best_val_accuracy = train_deberta_model(\n",
    "                model=model,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                optimizer=optimizer,\n",
    "                criterion=criterion,\n",
    "                epochs=EPOCHS,\n",
    "                patience=hyperparams['patience'],\n",
    "                trial=trial\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå DeBERTa Trial {trial.number} failed: {e}\")\n",
    "            best_val_accuracy = 0.0\n",
    "        finally:\n",
    "            wandb.finish()\n",
    "            # Clean up GPU memory\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        return best_val_accuracy\n",
    "\n",
    "    def train_deberta_model(model, train_loader, val_loader, optimizer, \n",
    "                           criterion, epochs, patience, trial):\n",
    "        \"\"\"\n",
    "        Train DeBERTa model with given hyperparameters and save the best checkpoint\n",
    "        \"\"\"\n",
    "        print(f\" Starting DeBERTa training for trial {trial.number}\")\n",
    "        \n",
    "        best_val_accuracy = 0.0\n",
    "        best_val_accuracy_epoch = 0\n",
    "        early_stop_flag = False\n",
    "        best_model_state = None\n",
    "        \n",
    "        for epoch in range(1, epochs + 1):\n",
    "            # =============== TRAINING PHASE ===============\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            total_train_samples = 0\n",
    "            correct_train_predictions = 0\n",
    "            \n",
    "            for batch in train_loader:\n",
    "                # Move data to device\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Accumulate metrics\n",
    "                train_loss += loss.item() * input_ids.size(0)\n",
    "                total_train_samples += input_ids.size(0)\n",
    "                correct_train_predictions += (logits.argmax(dim=1) == labels).sum().item()\n",
    "            \n",
    "            # Calculate training metrics\n",
    "            train_loss /= total_train_samples\n",
    "            train_accuracy = correct_train_predictions / total_train_samples\n",
    "            \n",
    "            # =============== VALIDATION PHASE ===============\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            total_val_samples = 0\n",
    "            correct_val_predictions = 0\n",
    "            all_val_labels = []\n",
    "            all_val_preds = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    # Move data to device\n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch['attention_mask'].to(device)\n",
    "                    labels = batch['labels'].to(device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                    logits = outputs.logits\n",
    "                    loss = criterion(logits, labels)\n",
    "                    \n",
    "                    # Accumulate metrics\n",
    "                    val_loss += loss.item() * input_ids.size(0)\n",
    "                    total_val_samples += input_ids.size(0)\n",
    "                    correct_val_predictions += (logits.argmax(dim=1) == labels).sum().item()\n",
    "                    all_val_labels.extend(labels.cpu().numpy())\n",
    "                    all_val_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "            \n",
    "            # Calculate validation metrics\n",
    "            val_loss /= total_val_samples\n",
    "            val_accuracy = correct_val_predictions / total_val_samples\n",
    "            val_precision = precision_score(all_val_labels, all_val_preds, average='weighted', zero_division=0)\n",
    "            val_recall = recall_score(all_val_labels, all_val_preds, average='weighted', zero_division=0)\n",
    "            val_f1 = f1_score(all_val_labels, all_val_preds, average='weighted', zero_division=0)\n",
    "            \n",
    "            # =============== EARLY STOPPING CHECK ===============\n",
    "            best_val_accuracy, best_val_accuracy_epoch, early_stop_flag = early_stop_check(\n",
    "                patience, best_val_accuracy, best_val_accuracy_epoch, val_accuracy, epoch\n",
    "            )\n",
    "            \n",
    "            # Save best model state\n",
    "            if val_accuracy == best_val_accuracy:\n",
    "                best_model_state = model.state_dict()\n",
    "            \n",
    "            # =============== LOGGING ===============\n",
    "            metrics = {\n",
    "                \"Epoch\": epoch,\n",
    "                \"Train Loss\": train_loss,\n",
    "                \"Train Accuracy\": train_accuracy,\n",
    "                \"Validation Loss\": val_loss,\n",
    "                \"Validation Accuracy\": val_accuracy,\n",
    "                \"Validation Precision\": val_precision,\n",
    "                \"Validation Recall\": val_recall,\n",
    "                \"Validation F1\": val_f1\n",
    "            }\n",
    "            wandb.log(metrics)\n",
    "            \n",
    "            # Print progress\n",
    "            if epoch % 5 == 0 or early_stop_flag:\n",
    "                print(f\"  Epoch {epoch}/{epochs}: Val Acc = {val_accuracy:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "            \n",
    "            # Check early stopping\n",
    "            if early_stop_flag:\n",
    "                print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        # =============== SAVE CHECKPOINT ===============\n",
    "        if best_model_state is not None:\n",
    "            save_deberta_checkpoint(trial, best_model_state, best_val_accuracy)\n",
    "        \n",
    "        print(f\"DeBERTa Trial {trial.number} completed: Best Val Acc = {best_val_accuracy:.4f}\")\n",
    "        return best_val_accuracy\n",
    "\n",
    "    def save_deberta_checkpoint(trial, best_model_state, best_val_accuracy):\n",
    "        \"\"\"\n",
    "        Save checkpoint for a specific DeBERTa trial\n",
    "        \"\"\"\n",
    "        checkpoint_dir = f\"{DEBERTA_CHECKPOINT_DIR}/trial_{trial.number}\"\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        checkpoint_data = {\n",
    "            'model_state_dict': best_model_state,\n",
    "            'trial_number': trial.number,\n",
    "            'best_val_accuracy': best_val_accuracy,\n",
    "            'hyperparameters': {\n",
    "                'learning_rate': trial.params.get('learning_rate'),\n",
    "                'weight_decay': trial.params.get('weight_decay'),\n",
    "                'patience': trial.params.get('patience'),\n",
    "                'batch_size': trial.params.get('batch_size'),\n",
    "                'num_layers': trial.params.get('num_layers')\n",
    "            },\n",
    "            'model_name': DEBERTA_MODEL_NAME,\n",
    "            'num_labels': NUM_LABELS,\n",
    "            'timestamp': time.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "        }\n",
    "        \n",
    "        checkpoint_path = f\"{checkpoint_dir}/model_checkpoint.pt\"\n",
    "        torch.save(checkpoint_data, checkpoint_path)\n",
    "        \n",
    "        print(f\"DeBERTa Trial {trial.number}: Checkpoint saved to {checkpoint_path} (Accuracy: {best_val_accuracy:.4f})\")\n",
    "        return checkpoint_path\n",
    "\n",
    "    def save_deberta_best_model(study):\n",
    "        \"\"\"\n",
    "        Save the best DeBERTa model from the completed study\n",
    "        \"\"\"\n",
    "        print(\" SAVING DEBERTA STUDY BEST MODEL\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        best_trial_num = study.best_trial.number\n",
    "        best_checkpoint_path = f\"{DEBERTA_CHECKPOINT_DIR}/trial_{best_trial_num}/model_checkpoint.pt\"\n",
    "        \n",
    "        if os.path.exists(best_checkpoint_path):\n",
    "            # Load the best trial's checkpoint\n",
    "            best_checkpoint = torch.load(best_checkpoint_path, map_location=device)\n",
    "            \n",
    "            # Create study-level best model directory\n",
    "            study_best_dir = f\"{DEBERTA_CHECKPOINT_DIR}/best_model\"\n",
    "            os.makedirs(study_best_dir, exist_ok=True)\n",
    "            \n",
    "            # Add study-level metadata\n",
    "            study_best_checkpoint = {\n",
    "                **best_checkpoint,\n",
    "                'study_name': DEBERTA_STUDY_NAME,\n",
    "                'study_best_trial': study.best_trial.number,\n",
    "                'study_best_value': study.best_value,\n",
    "                'total_trials': len(study.trials),\n",
    "                'optimization_completed': time.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "            }\n",
    "            \n",
    "            # Save the study's best model\n",
    "            study_best_path = f\"{study_best_dir}/best_model.pt\"\n",
    "            torch.save(study_best_checkpoint, study_best_path)\n",
    "            \n",
    "            print(f\"DeBERTa study best model saved:\")\n",
    "            print(f\"  Location: {study_best_path}\")\n",
    "            print(f\"  Accuracy: {study.best_value:.4f}\")\n",
    "            print(f\"  From trial: {study.best_trial.number}\")\n",
    "            \n",
    "            return study_best_path\n",
    "        else:\n",
    "            print(f\"‚ùå Best DeBERTa trial checkpoint not found at {best_checkpoint_path}\")\n",
    "            return None\n",
    "\n",
    "    def display_deberta_results(study):\n",
    "        \"\"\"\n",
    "        Display comprehensive DeBERTa optimization results\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\" DEBERTA OPTIMIZATION COMPLETED\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Basic results\n",
    "        print(f\"Best trial: {study.best_trial.number}\")\n",
    "        print(f\"Best accuracy: {study.best_value:.4f}\")\n",
    "        print(f\"Total trials: {len(study.trials)}\")\n",
    "        print(f\"Completed trials: {len([t for t in study.trials if t.value is not None])}\")\n",
    "        \n",
    "        # Best hyperparameters\n",
    "        print(f\"\\nBest hyperparameters:\")\n",
    "        for key, value in study.best_params.items():\n",
    "            if isinstance(value, float) and value < 0.001:\n",
    "                print(f\"  {key:15s}: {value:.2e}\")\n",
    "            else:\n",
    "                print(f\"  {key:15s}: {value}\")\n",
    "        \n",
    "        # Trial performance summary\n",
    "        trial_values = [trial.value for trial in study.trials if trial.value is not None]\n",
    "        if trial_values:\n",
    "            print(f\"\\nPerformance summary:\")\n",
    "            print(f\"  Best accuracy:    {max(trial_values):.4f}\")\n",
    "            print(f\"  Average accuracy: {np.mean(trial_values):.4f}\")\n",
    "            print(f\"  Std deviation:    {np.std(trial_values):.4f}\")\n",
    "        \n",
    "        # Checkpoint folder structure\n",
    "        print(f\"\\nDeBERTa Checkpoint folder structure:\")\n",
    "        print(f\"{DEBERTA_CHECKPOINT_DIR}/\")\n",
    "        print(f\"‚îú‚îÄ‚îÄ best_model/\")\n",
    "        print(f\"‚îÇ   ‚îî‚îÄ‚îÄ best_model.pt           # üèÜ DeBERTa Study's best model\")\n",
    "        print(f\"‚îú‚îÄ‚îÄ trial_0/\")\n",
    "        print(f\"‚îÇ   ‚îî‚îÄ‚îÄ model_checkpoint.pt     # üì¶ Individual trial checkpoints\")\n",
    "        print(f\"‚îú‚îÄ‚îÄ trial_1/\")\n",
    "        print(f\"‚îÇ   ‚îî‚îÄ‚îÄ model_checkpoint.pt\")\n",
    "        print(f\"‚îî‚îÄ‚îÄ ...\")\n",
    "        \n",
    "        return study.best_value, study.best_params\n",
    "\n",
    "    # =============== RUN DEBERTA OPTIMIZATION ===============\n",
    "    print(\"STARTING DEBERTA HYPERPARAMETER OPTIMIZATION\")\n",
    "    \n",
    "    # Clean up any previous wandb runs\n",
    "    wandb.finish()\n",
    "    \n",
    "    # Create and run DeBERTa study\n",
    "    deberta_study = optuna.create_study(direction=\"maximize\")\n",
    "    deberta_study.optimize(deberta_objective, n_trials=N_TRIALS)\n",
    "    \n",
    "    # =============== PROCESS DEBERTA RESULTS ===============\n",
    "    deberta_best_accuracy, deberta_best_params = display_deberta_results(deberta_study)\n",
    "    deberta_best_model_path = save_deberta_best_model(deberta_study)\n",
    "    \n",
    "    print(f\"\\nDeBERTa optimization and checkpoint saving completed!\")\n",
    "    print(f\"DeBERTa best model ready to load from: {deberta_best_model_path}\")\n",
    "    print(f\"DeBERTa best accuracy: {deberta_best_accuracy:.4f}\")\n",
    "    \n",
    "    # =============== MODEL COMPARISON (IF ROBERTA EXISTS) ===============\n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(\"MODEL COMPARISON SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check if RoBERTa study exists\n",
    "    roberta_checkpoint_path = \"checkpoints/roberta_hp_tuning_study/best_model/best_model.pt\"\n",
    "    if os.path.exists(roberta_checkpoint_path):\n",
    "        try:\n",
    "            roberta_checkpoint = torch.load(roberta_checkpoint_path, map_location='cpu')\n",
    "            roberta_accuracy = roberta_checkpoint['best_val_accuracy']\n",
    "            \n",
    "            print(f\"RoBERTa Model:\")\n",
    "            print(f\"    Best Accuracy: {roberta_accuracy:.4f}\")\n",
    "            print(f\"    Checkpoint: {roberta_checkpoint_path}\")\n",
    "            \n",
    "            print(f\"DeBERTa Model:\")\n",
    "            print(f\"   Best Accuracy: {deberta_best_accuracy:.4f}\")\n",
    "            print(f\"   Checkpoint: {deberta_best_model_path}\")\n",
    "            \n",
    "            # Declare winner\n",
    "            if deberta_best_accuracy > roberta_accuracy:\n",
    "                print(f\"\\nWINNER: DeBERTa (+{deberta_best_accuracy - roberta_accuracy:.4f})\")\n",
    "            elif roberta_accuracy > deberta_best_accuracy:\n",
    "                print(f\"\\nWINNER: RoBERTa (+{roberta_accuracy - deberta_best_accuracy:.4f})\")\n",
    "            else:\n",
    "                print(f\"\\nTIE: Both models achieved similar performance\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\" RoBERTa Model: Checkpoint exists but couldn't load ({e})\")\n",
    "            print(f\"DeBERTa Model:\")\n",
    "            print(f\"   Best Accuracy: {deberta_best_accuracy:.4f}\")\n",
    "            print(f\"   Checkpoint: {deberta_best_model_path}\")\n",
    "    else:\n",
    "        print(f\"RoBERTa Model: No checkpoint found\")\n",
    "        print(f\"DeBERTa Model:\")\n",
    "        print(f\"   Best Accuracy: {deberta_best_accuracy:.4f}\")\n",
    "        print(f\"   Checkpoint: {deberta_best_model_path}\")\n",
    "        print(f\"\\nDeBERTa model optimization completed successfully!\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed without data. Please run the data preprocessing cells first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8290480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best model on test set and save results\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "test_df = pd.read_csv('data/test_df.csv')\n",
    "print(f\"Loaded test set: {len(test_df)} samples\")\n",
    "\n",
    "# Load tokenizer and best model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "checkpoint_manager = CheckpointManager()\n",
    "best_model, checkpoint_data = checkpoint_manager.load_best_study_model()\n",
    "\n",
    "if best_model is not None:\n",
    "    # Prepare test dataset\n",
    "    class TestDataset(Dataset):\n",
    "        def __init__(self, df, tokenizer, max_length=MAX_LENGTH):\n",
    "            self.texts = df['CleanTweet'].tolist()\n",
    "            self.labels = df['label'].tolist() if 'label' in df.columns else None\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "        def __getitem__(self, idx):\n",
    "            encoding = self.tokenizer(\n",
    "                self.texts[idx],\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "            if self.labels:\n",
    "                item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "            return item\n",
    "\n",
    "    test_dataset = TestDataset(test_df, tokenizer)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    best_model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            outputs = best_model(input_ids, attention_mask=attention_mask)\n",
    "            preds = outputs.logits.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            if 'labels' in batch:\n",
    "                all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    # Save predictions\n",
    "    test_df['predicted_label'] = all_preds\n",
    "    test_df['predicted_sentiment'] = test_df['predicted_label'].map(ID2LABEL)\n",
    "    test_df.to_csv('test_predictions.csv', index=False)\n",
    "    print(\"Test predictions saved to test_predictions.csv\")\n",
    "\n",
    "    # Print metrics if labels available\n",
    "    if all_labels:\n",
    "        print(\"Test set classification report:\")\n",
    "        print(classification_report(all_labels, all_preds, target_names=[ID2LABEL[i] for i in range(NUM_LABELS)]))\n",
    "        print(f\"Test set accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "else:\n",
    "    print(\"Best model could not be loaded for test set evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63503bf9-e821-46da-a4c8-bfd9acb8406b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test set results to CSV\n",
    "import pandas as pd\n",
    "if 'test_df' in globals() and 'predicted_label' in test_df.columns:\n",
    "    test_df.to_csv('test_set_results_roberta_manual.csv', index=False)\n",
    "    print(\"Test set results saved to test_set_results_roberta_manual.csv\")\n",
    "else:\n",
    "    print(\"No test set predictions found to save.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
